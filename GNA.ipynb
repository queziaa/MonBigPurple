{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# 初始化BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# 简单的字符级别操作函数\n",
    "def add_typo(sentence, num_typos=1):\n",
    "    chars = list(sentence)\n",
    "    for _ in range(num_typos):\n",
    "        idx = random.randint(0, len(chars) - 1)\n",
    "        typo_type = random.choice(['delete', 'add', 'swap'])\n",
    "        if typo_type == 'delete':\n",
    "            chars.pop(idx)\n",
    "        elif typo_type == 'add':\n",
    "            chars.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "        elif typo_type == 'swap' and len(chars) > 1:\n",
    "            if idx == len(chars) - 1:\n",
    "                idx -= 1\n",
    "            chars[idx], chars[idx + 1] = chars[idx + 1], chars[idx]\n",
    "    return ''.join(chars)\n",
    "\n",
    "# 生成器网络\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return add_typo(x)\n",
    "\n",
    "# 判别器网络\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs[0])\n",
    "        return logits\n",
    "\n",
    "# 纠错器网络\n",
    "class Corrector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Corrector, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "# 初始化网络\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "corrector = Corrector()\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "optimizer_C = torch.optim.Adam(corrector.parameters(), lr=0.001)\n",
    "\n",
    "# 样本数据\n",
    "real_sentence = \"This is a correct sentence.\"\n",
    "fake_sentence = generator(real_sentence)\n",
    "\n",
    "# 训练判别器\n",
    "def train_discriminator(real_sentence, fake_sentence):\n",
    "    real_input = tokenizer(real_sentence, return_tensors='pt')\n",
    "    fake_input = tokenizer(fake_sentence, return_tensors='pt')\n",
    "    real_labels = torch.ones((1, 1))\n",
    "    fake_labels = torch.zeros((1, 1))\n",
    "\n",
    "    optimizer_D.zero_grad()\n",
    "    real_output = discriminator(real_input['input_ids'], real_input['attention_mask'])\n",
    "    fake_output = discriminator(fake_input['input_ids'], fake_input['attention_mask'])\n",
    "\n",
    "    loss_real = criterion(real_output, real_labels)\n",
    "    loss_fake = criterion(fake_output, fake_labels)\n",
    "    loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "    loss_D.backward()\n",
    "    optimizer_D.step()\n",
    "    return loss_D.item()\n",
    "\n",
    "# 训练生成器\n",
    "def train_generator(fake_sentence):\n",
    "    fake_input = tokenizer(fake_sentence, return_tensors='pt')\n",
    "    real_labels = torch.ones((1, 1))\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "    fake_output = discriminator(fake_input['input_ids'], fake_input['attention_mask'])\n",
    "\n",
    "    loss_G = criterion(fake_output, real_labels)\n",
    "\n",
    "    loss_G.backward()\n",
    "    optimizer_G.step()\n",
    "    return loss_G.item()\n",
    "\n",
    "# 训练纠错器\n",
    "def train_corrector(fake_sentence, real_sentence):\n",
    "    fake_input = tokenizer(fake_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    real_input = tokenizer(real_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    optimizer_C.zero_grad()\n",
    "    outputs = corrector(fake_input['input_ids'], fake_input['attention_mask'], labels=real_input['input_ids'])\n",
    "    loss_C = outputs.loss\n",
    "\n",
    "    loss_C.backward()\n",
    "    optimizer_C.step()\n",
    "    return loss_C.item()\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    fake_sentence = generator(real_sentence)\n",
    "    loss_D = train_discriminator(real_sentence, fake_sentence)\n",
    "    loss_G = train_generator(fake_sentence)\n",
    "    loss_C = train_corrector(fake_sentence, real_sentence)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss D: {loss_D}, Loss G: {loss_G}, Loss C: {loss_C}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
