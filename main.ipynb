{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_spell_error.txt\n",
    "# train_clean.txt\n",
    "spell_error = open(\"train_spell_error.txt\", \"r\")\n",
    "clean = open(\"train_clean.txt\", \"r\")\n",
    "spell_error = spell_error.read().split(\"\\n\")\n",
    "clean = clean.read().split(\"\\n\")\n",
    "\n",
    "def letterSim(A,B):\n",
    "    tedict = {}\n",
    "    for i in range(len(A)):\n",
    "        if A[i] not in tedict:\n",
    "            tedict[A[i]] = 1\n",
    "        else:\n",
    "            tedict[A[i]] += 1\n",
    "    for i in range(len(B)):\n",
    "        if B[i] not in tedict:\n",
    "            tedict[B[i]] = -1\n",
    "        else:\n",
    "            tedict[B[i]] -= 1\n",
    "    sum = 0\n",
    "    for i in tedict:\n",
    "        sum += abs(tedict[i])\n",
    "    to = len(A) + len(B)\n",
    "    to = to / 2\n",
    "    return to,sum\n",
    "\n",
    "# 将spell_error和clean的数据每个句子以空格切分，视为单词，每个单词最后一个元素为“:”则将其单独切分\n",
    "for i in range(len(spell_error)):\n",
    "    temp = spell_error[i].split(\" \")\n",
    "    spell_error[i] = []\n",
    "    for j in range(len(temp)):\n",
    "        # print(temp)\n",
    "        # print('*',temp[j],'*')\n",
    "        if len(temp[j]) == 0:\n",
    "            continue\n",
    "        if temp[j][-1] == \":\":\n",
    "            spell_error[i].append(temp[j][:-1])\n",
    "            spell_error[i].append(\":\")\n",
    "        else:\n",
    "            spell_error[i].append(temp[j])\n",
    "\n",
    "for i in range(len(clean)):\n",
    "    temp = clean[i].split(\" \")\n",
    "    clean[i] = []\n",
    "    for j in range(len(temp)):\n",
    "        if len(temp[j]) == 0:\n",
    "            continue\n",
    "        if temp[j][-1] == \":\":\n",
    "            clean[i].append(temp[j][:-1])\n",
    "            clean[i].append(\":\")\n",
    "        else:\n",
    "            clean[i].append(temp[j])\n",
    "\n",
    "for i in range(len(spell_error)):\n",
    "    if len(spell_error[i]) != len(clean[i]):\n",
    "        for j in range(len(spell_error[i])):\n",
    "            # 如果含有“:”且不是最后一个元素\n",
    "            if spell_error[i][j].find(\":\") != -1 and spell_error[i][j][-1] != \":\":\n",
    "                # spell_error[i][j]位置赋予删除“:”后的单词\n",
    "                spell_error[i][j] = spell_error[i][j].replace(\":\", \"\")\n",
    "                # spell_error[i][j]之后插入“:”\n",
    "                spell_error[i].insert(j+1, \":\")\n",
    "\n",
    "\n",
    "OUT = {}\n",
    "# 错误绝大部分都是对应位置的单词的拼写错误 \n",
    "ta = {}\n",
    "tb = {}\n",
    "errerDict = {}\n",
    "norDict = {}\n",
    "tol = 0  #79791\n",
    "onesenhavemulerrors = 0   #22883\n",
    "senlenserr = 0  #34\n",
    "for i in range(len(spell_error)):\n",
    "    indexx = 0\n",
    "    for  j in range(len(spell_error[i])):\n",
    "        if len(spell_error[i]) == len(clean[i]) and spell_error[i][j] != clean[i][j]:\n",
    "            tol += 1\n",
    "            indexx += 1\n",
    "\n",
    "        #     if spell_error[i][j] not in errerDict:\n",
    "        #         errerDict[spell_error[i][j]] = 1\n",
    "        #     else:\n",
    "        #         errerDict[spell_error[i][j]] += 1\n",
    "\n",
    "        #     if clean[i][j] not in norDict:\n",
    "        #         norDict[clean[i][j]] = 1\n",
    "        #     else:\n",
    "        #         norDict[clean[i][j]] += 1\n",
    "        # else:\n",
    "        #     if clean[i][j] not in norDict:\n",
    "        #         norDict[clean[i][j]] = 1\n",
    "        #     else:\n",
    "        #         norDict[clean[i][j]] += 1\n",
    "        \n",
    "\n",
    "\n",
    "            # if len(spell_error[i][j]) == 1:\n",
    "            #     print(spell_error[i][j],clean[i][j])\n",
    "            #     if spell_error[i][j] not in ta:\n",
    "            #         ta[spell_error[i][j]] = 1\n",
    "            #     else:\n",
    "            #         ta[spell_error[i][j]] += 1\n",
    "\n",
    "            # if len(clean[i][j]) == 1:\n",
    "            #     print(spell_error[i][j],clean[i][j])\n",
    "            #     if clean[i][j] not in tb:\n",
    "            #         tb[clean[i][j]] = 1\n",
    "            #     else:\n",
    "            #         tb[clean[i][j]] += 1\n",
    "\n",
    "            # a,b = letterSim(clean[i][j],spell_error[i][j])\n",
    "            # if b > a :\n",
    "            #     print('#',clean[i][j],'#',spell_error[i][j],'#',letterSim(clean[i][j],spell_error[i][j]))\n",
    "\n",
    "\n",
    "            # print(spell_error[i])\n",
    "            # print(clean[i])\n",
    "            # print(\" \".join(clean[i]))\n",
    "            # print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    if len(spell_error[i]) != len(clean[i]):\n",
    "        # print(spell_error[i])\n",
    "        # print(clean[i])\n",
    "        # print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "        senlenserr +=1 \n",
    "    # if indexx > 1:\n",
    "        # onesenhavemulerrors += 1\n",
    "\n",
    "\n",
    "\n",
    "# # 寻找所有\":\"之前的单词  \n",
    "# te = {}\n",
    "# for i in range(len(clean)):\n",
    "#     for j in range(len(clean[i])-1):\n",
    "#         if clean[i][j+1] == \":\":\n",
    "#             if clean[i][j] not in te:\n",
    "#                 te[clean[i][j]] = 1\n",
    "#             else:\n",
    "#                 te[clean[i][j]] += 1\n",
    "\n",
    "# for i in range(len(clean)):\n",
    "#     for j in range(len(clean[i])):\n",
    "#         if clean[i][j] in te and len(clean[i]) > j+1:\n",
    "#             print(clean[i][j+1])\n",
    "# 单字错误情况\n",
    "# я\t8\t——t\n",
    "# ю\t1\t\n",
    "# ю\t97\t——t\n",
    "# э\t1\t\n",
    "# э\t72\t——t\n",
    "# ь\t258\t——t\n",
    "# ы\t1\t\n",
    "# ы\t10\t——t\n",
    "# щ\t22\t——t\n",
    "# ш\t6\t\n",
    "# ш\t23\t——t\n",
    "# ч\t300\t\n",
    "# ч\t26\t——t\n",
    "# ц\t8\t\n",
    "# ц\t13\t——t\n",
    "# х\t4\t\n",
    "# х\t83\t——t\n",
    "# ф\t3\t\n",
    "# ф\t18\t——t\n",
    "# ү\t59\t——t\n",
    "# у\t1\t\n",
    "# у\t64\t——t\n",
    "# т\t10\t\n",
    "# т\t37\t——t\n",
    "# с\t13\t\n",
    "# с\t46\t——t\n",
    "# р\t123\t\n",
    "# р\t44\t——t\n",
    "# п\t2\t\n",
    "# п\t14\t——t\n",
    "# ө\t6\t\n",
    "# ө\t4\t——t\n",
    "# о\t2\t\n",
    "# о\t7\t——t\n",
    "# н\t9\t\n",
    "# н\t305\t——t\n",
    "# м\t8\t\n",
    "# м\t121\t——t\n",
    "# л\t221\t\n",
    "# л\t28\t——t\n",
    "# к\t2\t\n",
    "# к\t16\t——t\n",
    "# й\t35\t——t\n",
    "# и\t3\t\n",
    "# и\t115\t——t\n",
    "# з\t2\t\n",
    "# з\t29\t——t\n",
    "# ж\t4\t\n",
    "# ж\t38\t——t\n",
    "# ё\t6\t——t\n",
    "# е\t2\t\n",
    "# е\t16\t——t\n",
    "# д\t50\t\n",
    "# д\t28\t——t\n",
    "# г\t22\t\n",
    "# г\t49\t——t\n",
    "# в\t6\t\n",
    "# в\t37\t——t\n",
    "# б\t13\t\n",
    "# б\t102\t——t\n",
    "# а\t3\t\n",
    "# v\t2\t\n",
    "# s\t1\t\n",
    "# n\t1\t\n",
    "# i\t1\t\n",
    "# 5\t2\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordsDict.pickle  读取\n",
    "import pickle\n",
    "with open('wordsDict.pickle', 'rb') as f:\n",
    "    wordsDict = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56962"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "norDict\n",
    "len(errerDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errrepeat = 0  #9578个词是句子中错误拼写但是在正确的词典中\n",
    "for i in errerDict:\n",
    "    if i in norDict:\n",
    "        errrepeat += errerDict[i]\n",
    "        print(i,errerDict[i],norDict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errrepeat = 0  #41968 个词是句子中错误拼写但是在正确的词典中 这次是更大词典\n",
    "for i in errerDict:\n",
    "    if i not in wordsDict:\n",
    "        print(i,errerDict[i])\n",
    "        errrepeat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41968"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errrepeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56962"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errerDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2943d59039cd477fba977b006fc082b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\quezi\\.cache\\huggingface\\hub\\models--tugstugi--bert-large-mongolian-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ff47ace1fb4bd7a0949926559e1c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4de54f827ee4e869c330c5b5b4448d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23abaac95f734d01b5f8d9439ef44be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7b533e4398457c884ca56b68d8094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf08101e07a7473280a431312fd90940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9779230356216431, 'token': 1176, 'token_str': 'нийслэл', 'sequence': 'Монгол улсын нийслэл Улаанбаатар хотоос ярьж байна.'}\n",
      "{'score': 0.015034747309982777, 'token': 4059, 'token_str': 'Нийслэл', 'sequence': 'Монгол улсын Нийслэл Улаанбаатар хотоос ярьж байна.'}\n",
      "{'score': 0.002141402568668127, 'token': 325, 'token_str': 'Ерөнхийлөгч', 'sequence': 'Монгол улсын Ерөнхийлөгч Улаанбаатар хотоос ярьж байна.'}\n",
      "{'score': 0.0008035437786020339, 'token': 1215, 'token_str': 'ерөнхийлөгч', 'sequence': 'Монгол улсын ерөнхийлөгч Улаанбаатар хотоос ярьж байна.'}\n",
      "{'score': 0.0006434023380279541, 'token': 356, 'token_str': 'нийслэлийн', 'sequence': 'Монгол улсын нийслэлийн Улаанбаатар хотоос ярьж байна.'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-cased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-cased')\n",
    "\n",
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Монгол улсын [MASK] Улаанбаатар хотоос ярьж байна.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i])\n",
    "\n",
    "## output ##\n",
    "# {'sequence': 'Монгол улсын нийслэл Улаанбаатар хотоос ярьж байна.', 'score': 0.9779232740402222, 'token': 1176, 'token_str': 'нийслэл'}\n",
    "# {'sequence': 'Монгол улсын Нийслэл Улаанбаатар хотоос ярьж байна.', 'score': 0.015034765936434269, 'token': 4059, 'token_str': 'Нийслэл'}\n",
    "# {'sequence': 'Монгол улсын Ерөнхийлөгч Улаанбаатар хотоос ярьж байна.', 'score': 0.0021413620561361313, 'token': 325, 'token_str': 'Ерөнхийлөгч'}\n",
    "# {'sequence': 'Монгол улсын ерөнхийлөгч Улаанбаатар хотоос ярьж байна.', 'score': 0.0008035294013097882, 'token': 1215, 'token_str': 'ерөнхийлөгч'}\n",
    "# {'sequence': 'Монгол улсын нийслэлийн Улаанбаатар хотоос ярьж байна.', 'score': 0.0006434018723666668, 'token': 356, 'token_str': 'нийслэлийн'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bayartsogt/albert-mongolian\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bayartsogt/albert-mongolian\")\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bayartsogt/mongolian-roberta-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bayartsogt/mongolian-roberta-large\")\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "########################################################################################\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-base-mongolian-uncased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-base-mongolian-uncased')\n",
    "\n",
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Миний [MASK] хоол идэх нь тун чухал.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i])\n",
    "\n",
    "## output ##\n",
    "#{'sequence': 'миний хувьд хоол идэх нь тун чухал.', 'score': 0.7889143824577332, 'token': 126, 'token_str': 'хувьд'}\n",
    "#{'sequence': 'миний бодлоор хоол идэх нь тун чухал.', 'score': 0.18616807460784912, 'token': 6106, 'token_str': 'бодлоор'}\n",
    "#{'sequence': 'миний зүгээс хоол идэх нь тун чухал.', 'score': 0.004825591575354338, 'token': 761, 'token_str': 'зүгээс'}\n",
    "#{'sequence': 'миний биед хоол идэх нь тун чухал.', 'score': 0.0015743684489279985, 'token': 3010, 'token_str': 'биед'}\n",
    "#{'sequence': 'миний тухайд хоол идэх нь тун чухал.', 'score': 0.0014919431414455175, 'token': 1712, 'token_str': 'тухайд'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_file' from 'utils' (c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join, exists\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_file, sentence_tokenize\n\u001b[0;32m     10\u001b[0m MN_WIKI_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmn_wiki.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m MN_CORPUS_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmn_corpus\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'download_file' from 'utils' (c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Download and pre process the Mongolian Wikipedia.\"\"\"\n",
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from os.path import join, exists\n",
    "from utils import download_file, sentence_tokenize\n",
    "\n",
    "MN_WIKI_FILE = 'mn_wiki.txt'\n",
    "MN_CORPUS_FOLDER = 'mn_corpus'\n",
    "MN_WIKI_RAR_FILE = 'mn_wiki.bz2'\n",
    "MN_WIKI_EXTRACT_FOLDER = 'tmp_mn_wiki'\n",
    "MN_WIKI_URL = 'https://dumps.wikimedia.org/mnwiki/20181220/mnwiki-20181220-pages-articles-multistream.xml.bz2'\n",
    "\n",
    "\n",
    "# create corpus directory\n",
    "if not exists(MN_CORPUS_FOLDER):\n",
    "    os.makedirs(MN_CORPUS_FOLDER)\n",
    "\n",
    "# download\n",
    "if not exists(MN_WIKI_RAR_FILE):\n",
    "    download_file(MN_WIKI_URL, MN_WIKI_RAR_FILE)\n",
    "\n",
    "# extract\n",
    "os.system(\"python3 wikiextractor/WikiExtractor.py %s -o=%s\" % (MN_WIKI_RAR_FILE, MN_WIKI_EXTRACT_FOLDER))\n",
    "\n",
    "\n",
    "def _pre_process(wiki_file_name):\n",
    "    \"\"\"a very simple pre processing\"\"\"\n",
    "    print(\"pre processing '%s'...\" % wiki_file_name)\n",
    "    with open(wiki_file_name) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    # some articles has only 1 line, so write first into an array later filter them\n",
    "    articles = [[]]\n",
    "\n",
    "    article_start = False\n",
    "    # process line by line\n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "\n",
    "        # ignore empty line\n",
    "        if not line:\n",
    "            continue\n",
    "        # ignore article start\n",
    "        if line.startswith('<doc'):\n",
    "            article_start = True\n",
    "            continue\n",
    "        # after article start, there is the article title, ignore it\n",
    "        if article_start:\n",
    "            article_start = False\n",
    "            continue\n",
    "        # ignore categories\n",
    "        if line.startswith('[['):\n",
    "            continue\n",
    "        if line.startswith('</doc>'):\n",
    "            articles.append([])\n",
    "            continue\n",
    "        # tokenize the news article sentence by sentence\n",
    "        for sentence in sentence_tokenize(line):\n",
    "            articles[-1].append(sentence)\n",
    "\n",
    "    # we need articles with at least 2 lines\n",
    "    articles = [d for d in articles if len(d) >= 2]\n",
    "\n",
    "    # write into a single file\n",
    "    with open(join(MN_CORPUS_FOLDER, MN_WIKI_FILE), 'a') as f:\n",
    "        for article in articles:\n",
    "            for sentence in article:\n",
    "                f.write(sentence)\n",
    "                f.write('\\n')\n",
    "            # extra new line to separate each news articles\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "# pre process the extracted news files\n",
    "print('%s/*/*.txt' % MN_WIKI_EXTRACT_FOLDER)\n",
    "for file_name in glob.glob('%s/*/wiki_*' % MN_WIKI_EXTRACT_FOLDER):\n",
    "    _pre_process(file_name)\n",
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
