{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "from tqdm import tqdm\n",
    "# from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "# from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import colon,process_list\n",
    "from MonBigTool import MonBigTool,MASKmodel \n",
    "# WordsDict = monBigTool.getWordsDict()\n",
    "MODELNAME = 'tugstugi/bert-large-mongolian-uncased'\n",
    "\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "class fineTuningClass():\n",
    "    def __init__(self,fineT,tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 加载模型和分词器\n",
    "        self.model = BertForTokenClassification.from_pretrained(fineT)\n",
    "        # model = BertForTokenClassification.from_pretrained('./RRRRRRRRRRR')\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "    def predict(self,sen):\n",
    "        sentence = ' '.join(sen)\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs.logits.argmax(-1).tolist()\n",
    "        # print(predictions)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "        sen = sentence.split()\n",
    "        Tindex = 0\n",
    "        SecondaryTreatment = []\n",
    "        for j in range(len(sen)):\n",
    "            aAaaa = tokenizer.tokenize(sen[j])\n",
    "            # print(sen[j],len(aAaaa),predictions[0][Tindex:Tindex+len(aAaaa)])\n",
    "            if all(v == 0 for v in predictions[0][Tindex:Tindex+len(aAaaa)]) == False:\n",
    "                SecondaryTreatment.append(j)\n",
    "            Tindex = Tindex + len(aAaaa)\n",
    "        return SecondaryTreatment\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MonBigPurple():\n",
    "    def __init__(self,fineTuningClass = None,fineFile = None):\n",
    "        self.WINDOW = 3\n",
    "        self.mASKmodel = MASKmodel(MODELNAME)\n",
    "        self.monBigTool = MonBigTool()\n",
    "        self.MASK = self.monBigTool.getMASK()\n",
    "        self.fineTuningClass = fineTuningClass\n",
    "        if self.fineTuningClass != None:\n",
    "            self.fineTuningClasspredict = fineTuningClass(fineFile,self.mASKmodel.tokenizer)\n",
    "\n",
    "        # self.LOSS_PScore = 0\n",
    "        # self.LOSS_PError = 0\n",
    "        # self.LOSS_PMiss = 0\n",
    "        # self.LOSS_PTrue = 0\n",
    "        # self.LOSS_PTrue_errFix = 0\n",
    "        # self.LOSS_PTrue_succFix = 0\n",
    "        # self.LOSS_O_T = 0\n",
    "        # self.LOSS_O_F = 0\n",
    "        # self.LOSS_O_MISS = 0\n",
    "    \n",
    "    def predict(self,fsen):\n",
    "\n",
    "        # inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "        # ########################################\n",
    "        # # 得到词对 信息  ########################\n",
    "        # for g in pairs:\n",
    "        #     if not g[0].isalpha():\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "        #     # if te < 0.8:\n",
    "        #         # print(g[0],g[1],te)\n",
    "        # ########################################\n",
    "\n",
    "        SecondaryTreatment = []\n",
    "\n",
    "\n",
    "        if self.fineTuningClass != None:\n",
    "            SecondaryTreatment = self.fineTuningClasspredict.predict(fsen)\n",
    "        else:\n",
    "            temp = self.mASKmodel.tomask(fsen,self.WINDOW,False)\n",
    "            for i in range(len(fsen)):\n",
    "                if not fsen[i].isalpha():\n",
    "                    continue\n",
    "                if fsen[i] in temp[i] :\n",
    "                    continue\n",
    "                if self.monBigTool.mysterious(fsen[i]):\n",
    "                    continue\n",
    "                # if i in inode:\n",
    "                #     LOSS_O_T += 1\n",
    "                #     inode.remove(i)\n",
    "                # else:\n",
    "                #     LOSS_O_F += 1\n",
    "                SecondaryTreatment.append(i)\n",
    "\n",
    "        \n",
    "        # LOSS_O_MISS += len(inode)\n",
    "        # print('假----',' '.join(fsen),'----')\n",
    "        # print('真----',' '.join(tsen),'----')\n",
    "        # print('MASK----',' '.join(sen),'----')\n",
    "        # print('怀疑----',SecondaryTreatment,'----',inode)\n",
    "        OUT = []\n",
    "        for i in SecondaryTreatment:\n",
    "            candidate,score = self.monBigTool.FuzzySearch(fsen[i])\n",
    "            Target = fsen[i]\n",
    "            TTEMP = self.mASKmodel.hybridPrediction(fsen,i,3,candidate,score)\n",
    "            if len(TTEMP) == 0:\n",
    "                continue\n",
    "            OUT.append({'i':i,'f':Target,'t':TTEMP[0]})\n",
    "        return OUT\n",
    "            # TTEMP = candidate\n",
    "        #     print('目标',Target,':  候选',candidate)\n",
    "        #     print('结果:',TTEMP)\n",
    "\n",
    "\n",
    "        #     if len(TTEMP) == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     if fsen[i] == TTEMP[0]:\n",
    "        #         continue\n",
    "        \n",
    "        #     if i not in inode:\n",
    "        #         LOSS_PError += 1\n",
    "        #         print('*ERR'*20)\n",
    "        #         print(Target,TTEMP)\n",
    "        #     elif i in inode:\n",
    "        #         LOSS_PTrue += 1\n",
    "        #         inode.remove(i)\n",
    "        #         if tsen[i] == TTEMP[0]:\n",
    "        #             print('*SUCFix'*20)\n",
    "        #             LOSS_PTrue_succFix += 1\n",
    "        #         else:\n",
    "        #             print('*ERRFix'*20)\n",
    "        #             LOSS_PTrue_errFix += 1\n",
    "        #         print(Target,TTEMP)\n",
    "        \n",
    "        # LOSS_PMiss += len(inode)\n",
    "        # for i in inode:\n",
    "        #     print('@MISS'*20)\n",
    "        #     print('----',' '.join(fsen),'----')\n",
    "        #     print('----',' '.join(tsen),'----')\n",
    "        #     print('----',' '.join(sen),'----')\n",
    "        #     print(fsen[i])\n",
    "        #     print(fsen[i])\n",
    "\n",
    "        # if COUNT % 50 == 0:\n",
    "        # # print('-----------------')\n",
    "        #     print('O_T:',LOSS_O_T)\n",
    "        #     print('O_F:',LOSS_O_F)\n",
    "        #     print('O_MISS:',LOSS_O_MISS)\n",
    "        # print('PError:',LOSS_PError)\n",
    "        # print('PMiss:',LOSS_PMiss)|\n",
    "        # print('PTrue:',LOSS_PTrue)\n",
    "        # print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "        # print('PTrue_errFix:',LOSS_PTrue_errFix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ . хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ .\n",
      "0.0 0.0 0.0 0.0\n",
      "-----------------\n",
      "цэрэг хамгаалагчид нь бас өөр хоорондоо маргалдан явуулж сурсан эрдээ олны дунд илэрхийлдэг . зэрэг хамгаалагчид нь бас өөр хоорондоо маргалдаан явуулж сурсан эрдмээ олны дунд илэрхийлдэг .\n",
      "0.015306122448979591 0.002255639097744361 0.12 0.017684210526315792\n",
      "-----------------\n",
      "хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө өнгийн шинж чанартай байдаг . хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө ангийн шинж чанартай байдаг .\n",
      "0.0044444444444444444 0.0007017543859649122 0.1 0.015000000000000003\n",
      "-----------------\n",
      "монгол эмээд үндсийг зүйл хамгийн арвин байдаг . монгол эмэнд үндсийн зүйл хамгийн арвин байдаг .\n",
      "0.03125 0.005208333333333333 0.125 0.019444444444444445\n",
      "-----------------\n",
      "нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг . нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг .\n",
      "0.0 0.0 0.10526315789473684 0.016374269005847954\n",
      "-----------------\n",
      "харин одоо миний нүд таныг хардаг . харин одоо миний нүд таныг хардаг .\n",
      "0.0 0.0 0.09375 0.014583333333333334\n",
      "-----------------\n",
      "эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ . эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ .\n",
      "0.0 0.0 0.0821917808219178 0.012785388127853882\n",
      "-----------------\n",
      "хаан та морио далайн дундах цээл арал дээр дасгаж . хаан та морио далайн дундах цээж арал дээр дасгав .\n",
      "0.02 0.00392156862745098 0.0963855421686747 0.01596976139853532\n",
      "-----------------\n",
      "энэ бодлогыг бүрэн дарснаар засгийн газар гуяа хүлээснээсээ сална . энэ бослогыг бүрэн дарснаар засгийн газар уяа хүлээснээсээ салав .\n",
      "0.03 0.006060606060606061 0.11827956989247312 0.02076936346386062\n",
      "-----------------\n",
      "эдгээр нь бизнес үргэлж хэрэгсээр ирсэн бөгөөд цаашид ч тэгэх болно . эдгээр нь бизнест үргэлж хэрэгжсээр ирсэн бөгөөд цаашид ч тэгэх болно .\n",
      "0.013888888888888888 0.002347417840375587 0.12380952380952381 0.021615037820505924\n",
      "-----------------\n",
      "чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов . чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов .\n",
      "0.0 0.0 0.11504424778761062 0.02008476965622232\n",
      "-----------------\n",
      "тэд мөн өөрийгөө үнэлэх үпэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг . тэд мөн өөрийгөө үнэлэх үнэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг .\n",
      "0.01 0.0014705882352941176 0.11382113821138211 0.019647461745386452\n",
      "-----------------\n",
      "гал хавдрын засахад уран хараацай үүрийн шавыг тйалхдаад тахианы өндөгний цагаанаар зуурч түрхдэг . гал хавдрыг засахад уран хараацайн үүрийн шаврыг талхдаад тахиан өндөгний цагаанаар зуурч түрхдэг .\n",
      "0.025510204081632654 0.0036075036075036075 0.1386861313868613 0.0228007919836003\n",
      "-----------------\n",
      "тэр шашин төрийн хавчлаганд орж намаа түр хаяж ч болно . тэр шажин төрийн хавчлаганд орж намайг түр хаяж ч болно .\n",
      "0.01652892561983471 0.004784688995215311 0.14189189189189189 0.025017945068745228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m temp \u001b[38;5;241m=\u001b[39m process_list(test_spell[i])\n\u001b[0;32m     18\u001b[0m temp \u001b[38;5;241m=\u001b[39m colon(temp)\n\u001b[1;32m---> 19\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m WORDLEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(temp)\n\u001b[0;32m     21\u001b[0m wordCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m WORDLEN\n",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m, in \u001b[0;36mMonBigPurple.predict\u001b[1;34m(self, fsen)\u001b[0m\n\u001b[0;32m    104\u001b[0m OUT \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m SecondaryTreatment:\n\u001b[1;32m--> 106\u001b[0m     candidate,score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonBigTool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFuzzySearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsen\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     Target \u001b[38;5;241m=\u001b[39m fsen[i]\n\u001b[0;32m    108\u001b[0m     TTEMP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmASKmodel\u001b[38;5;241m.\u001b[39mhybridPrediction(fsen,i,\u001b[38;5;241m3\u001b[39m,candidate,score)\n",
      "File \u001b[1;32mc:\\Users\\quezi\\MonBigPurple\\MonBigTool.py:143\u001b[0m, in \u001b[0;36mMonBigTool.FuzzySearch\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m wordDict:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wordDict[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m427\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "# te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "te = MonBigPurple()\n",
    "test_spell = open(\"train_spell_error.txt\", \"r\")\n",
    "train_clean = open(\"train_clean.txt\", \"r\")\n",
    "\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "train_clean = train_clean.read().split(\"\\n\")\n",
    "\n",
    "WERSUM = 0\n",
    "CERSUM = 0\n",
    "\n",
    "wordCount = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_spell)):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    WORDLEN = len(temp)\n",
    "    wordCount += WORDLEN\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    print('-----------------')    \n",
    "    Tw = wer(train_clean[i], temp)\n",
    "    WERSUM += Tw*WORDLEN\n",
    "    Tc =  cer(train_clean[i], temp)\n",
    "    CERSUM += Tc*WORDLEN\n",
    "\n",
    "    print(temp,train_clean[i])\n",
    "    print(Tw/WORDLEN,Tc/WORDLEN,WERSUM/wordCount,CERSUM/wordCount)\n",
    "\n",
    "# WER（Word Error Rate）\n",
    "# CER（Character Error Rate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "# test_spell = open(\"test_spell_error.txt\", \"r\")\n",
    "# test_spell = test_spell.read().split(\"\\n\")\n",
    "# OUT = []\n",
    "\n",
    "\n",
    "# for i in tqdm(range(len(test_spell))):\n",
    "#     temp = process_list(test_spell[i])\n",
    "#     temp = colon(temp)\n",
    "#     predict = te.predict(temp)\n",
    "#     for j in predict:\n",
    "#         t = j['t']\n",
    "#         ii = j['i']\n",
    "\n",
    "#         temp[ii] = t\n",
    "#     temp = ' '.join(temp)\n",
    "#     temp = temp.replace(' : ',': ')\n",
    "#     OUT.append(temp)\n",
    "#     print('-----------------')    \n",
    "#     print(temp)\n",
    "#     print(test_spell[i])\n",
    "#     print('-----------------')    \n",
    "\n",
    "# with open('test_spell_error_out.txt', 'w') as f:\n",
    "#     for item in OUT:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT = []\n",
    "# for i in tqdm(monBigTool.getMASK()):\n",
    "#     sen = []\n",
    "#     labels = []\n",
    "#     for ss in i['sen']:\n",
    "#         if ss == '<>':\n",
    "#             i['word'].pop(0)\n",
    "#             sen.append(i['word'].pop(0))\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [1] * count\n",
    "#         else:\n",
    "#             sen.append(ss)\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [0] * count\n",
    "#     OUT.append({\n",
    "#         'sen':sen,\n",
    "#         'labels':labels\n",
    "#     })\n",
    "# # 保存OUT\n",
    "# import json\n",
    "# with open('sentencePair.json','w') as f:\n",
    "#     json.dump(OUT,f)\n",
    "\n",
    "# # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)\n",
    "#     # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
