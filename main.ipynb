{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "from tqdm import tqdm\n",
    "# from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "# from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import colon,process_list\n",
    "from MonBigTool import MonBigTool,MASKmodel \n",
    "# WordsDict = monBigTool.getWordsDict()\n",
    "MODELNAME = 'tugstugi/bert-large-mongolian-uncased'\n",
    "\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "class fineTuningClass():\n",
    "    def __init__(self,fineT,tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 加载模型和分词器\n",
    "        self.model = BertForTokenClassification.from_pretrained(fineT)\n",
    "        # model = BertForTokenClassification.from_pretrained('./RRRRRRRRRRR')\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "    def predict(self,sen):\n",
    "        sentence = ' '.join(sen)\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs.logits.argmax(-1).tolist()\n",
    "        # print(predictions)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "        sen = sentence.split()\n",
    "        Tindex = 0\n",
    "        SecondaryTreatment = []\n",
    "        for j in range(len(sen)):\n",
    "            aAaaa = tokenizer.tokenize(sen[j])\n",
    "            # print(sen[j],len(aAaaa),predictions[0][Tindex:Tindex+len(aAaaa)])\n",
    "            if all(v == 0 for v in predictions[0][Tindex:Tindex+len(aAaaa)]) == False:\n",
    "                SecondaryTreatment.append(j)\n",
    "            Tindex = Tindex + len(aAaaa)\n",
    "        return SecondaryTreatment\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MonBigPurple():\n",
    "    def __init__(self,fineTuningClass = None,fineFile = None):\n",
    "        self.WINDOW = 3\n",
    "        self.mASKmodel = MASKmodel(MODELNAME)\n",
    "        self.monBigTool = MonBigTool()\n",
    "        self.MASK = self.monBigTool.getMASK()\n",
    "        self.fineTuningClass = fineTuningClass\n",
    "        if self.fineTuningClass != None:\n",
    "            self.fineTuningClasspredict = fineTuningClass(fineFile,self.mASKmodel.tokenizer)\n",
    "\n",
    "        # self.LOSS_PScore = 0\n",
    "        # self.LOSS_PError = 0\n",
    "        # self.LOSS_PMiss = 0\n",
    "        # self.LOSS_PTrue = 0\n",
    "        # self.LOSS_PTrue_errFix = 0\n",
    "        # self.LOSS_PTrue_succFix = 0\n",
    "        # self.LOSS_O_T = 0\n",
    "        # self.LOSS_O_F = 0\n",
    "        # self.LOSS_O_MISS = 0\n",
    "    \n",
    "    def predict(self,fsen):\n",
    "\n",
    "        # inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "        # ########################################\n",
    "        # # 得到词对 信息  ########################\n",
    "        # for g in pairs:\n",
    "        #     if not g[0].isalpha():\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "        #     # if te < 0.8:\n",
    "        #         # print(g[0],g[1],te)\n",
    "        # ########################################\n",
    "\n",
    "        SecondaryTreatment = []\n",
    "\n",
    "\n",
    "        if self.fineTuningClass != None:\n",
    "            SecondaryTreatment = self.fineTuningClasspredict.predict(fsen)\n",
    "        else:\n",
    "            temp = self.mASKmodel.tomask(fsen,self.WINDOW,False)\n",
    "            for i in range(len(fsen)):\n",
    "                if not fsen[i].isalpha():\n",
    "                    continue\n",
    "                if fsen[i] in temp[i] :\n",
    "                    continue\n",
    "                if self.monBigTool.mysterious(fsen[i]):\n",
    "                    continue\n",
    "                # if i in inode:\n",
    "                #     LOSS_O_T += 1\n",
    "                #     inode.remove(i)\n",
    "                # else:\n",
    "                #     LOSS_O_F += 1\n",
    "                SecondaryTreatment.append(i)\n",
    "\n",
    "        \n",
    "        # LOSS_O_MISS += len(inode)\n",
    "        # print('假----',' '.join(fsen),'----')\n",
    "        # print('真----',' '.join(tsen),'----')\n",
    "        # print('MASK----',' '.join(sen),'----')\n",
    "        # print('怀疑----',SecondaryTreatment,'----',inode)\n",
    "        OUT = []\n",
    "        for i in SecondaryTreatment:\n",
    "            candidate,score = self.monBigTool.FuzzySearch(fsen[i])\n",
    "            Target = fsen[i]\n",
    "            TTEMP = self.mASKmodel.hybridPrediction(fsen,i,3,candidate,score)\n",
    "            if len(TTEMP) == 0:\n",
    "                continue\n",
    "            OUT.append({'i':i,'f':Target,'t':TTEMP[0]})\n",
    "        return OUT\n",
    "            # TTEMP = candidate\n",
    "        #     print('目标',Target,':  候选',candidate)\n",
    "        #     print('结果:',TTEMP)\n",
    "\n",
    "\n",
    "        #     if len(TTEMP) == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     if fsen[i] == TTEMP[0]:\n",
    "        #         continue\n",
    "        \n",
    "        #     if i not in inode:\n",
    "        #         LOSS_PError += 1\n",
    "        #         print('*ERR'*20)\n",
    "        #         print(Target,TTEMP)\n",
    "        #     elif i in inode:\n",
    "        #         LOSS_PTrue += 1\n",
    "        #         inode.remove(i)\n",
    "        #         if tsen[i] == TTEMP[0]:\n",
    "        #             print('*SUCFix'*20)\n",
    "        #             LOSS_PTrue_succFix += 1\n",
    "        #         else:\n",
    "        #             print('*ERRFix'*20)\n",
    "        #             LOSS_PTrue_errFix += 1\n",
    "        #         print(Target,TTEMP)\n",
    "        \n",
    "        # LOSS_PMiss += len(inode)\n",
    "        # for i in inode:\n",
    "        #     print('@MISS'*20)\n",
    "        #     print('----',' '.join(fsen),'----')\n",
    "        #     print('----',' '.join(tsen),'----')\n",
    "        #     print('----',' '.join(sen),'----')\n",
    "        #     print(fsen[i])\n",
    "        #     print(fsen[i])\n",
    "\n",
    "        # if COUNT % 50 == 0:\n",
    "        # # print('-----------------')\n",
    "        #     print('O_T:',LOSS_O_T)\n",
    "        #     print('O_F:',LOSS_O_F)\n",
    "        #     print('O_MISS:',LOSS_O_MISS)\n",
    "        # print('PError:',LOSS_PError)\n",
    "        # print('PMiss:',LOSS_PMiss)\n",
    "        # print('PTrue:',LOSS_PTrue)\n",
    "        # print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "        # print('PTrue_errFix:',LOSS_PTrue_errFix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ./RRRRRRRRRRR were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ . хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ .\n",
      "0.0 0.0 0.0 0.0\n",
      "-----------------\n",
      "зэрэг хамгаалагчид нь бас өөр хоорондоо маргалдаан явуулж сурсан эрдмээ олны дунд илэрхийлдэг . зэрэг хамгаалагчид нь бас өөр хоорондоо маргалдаан явуулж сурсан эрдмээ олны дунд илэрхийлдэг .\n",
      "0.0 0.0 0.0 0.0\n",
      "-----------------\n",
      "хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө ангийн шинж чанартай байдаг . хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө ангийн шинж чанартай байдаг .\n",
      "0.0 0.0 0.0 0.0\n",
      "-----------------\n",
      "монгол эмэнд үндсийг зүйл хамгийн арвин байдаг . монгол эмэнд үндсийн зүйл хамгийн арвин байдаг .\n",
      "0.125 0.020833333333333332 0.03125 0.005208333333333333\n",
      "-----------------\n",
      "нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг . нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг .\n",
      "0.0 0.0 0.025 0.004166666666666667\n",
      "-----------------\n",
      "харин одоо миний нүд таныг хардаг . харин одоо миний нүд таныг хардаг .\n",
      "0.0 0.0 0.020833333333333332 0.003472222222222222\n",
      "-----------------\n",
      "эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ . эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ .\n",
      "0.0 0.0 0.017857142857142856 0.002976190476190476\n",
      "-----------------\n",
      "хаан та морио далайн дундах цээл арал дээр дасгав . хаан та морио далайн дундах цээж арал дээр дасгав .\n",
      "0.1 0.0196078431372549 0.028125 0.005055147058823529\n",
      "-----------------\n",
      "энэ бослогыг бүрэн дарснаар засгийн газар уяа хүлээснээсээ салав . энэ бослогыг бүрэн дарснаар засгийн газар уяа хүлээснээсээ салав .\n",
      "0.0 0.0 0.025 0.004493464052287581\n",
      "-----------------\n",
      "эдгээр н бизнес үргэлж хэрэгжсээр ирсэн бөгөөд цаашид ч тэгэх болно . эдгээр нь бизнест үргэлж хэрэгжсээр ирсэн бөгөөд цаашид ч тэгэх болно .\n",
      "0.16666666666666666 0.028169014084507043 0.03916666666666667 0.006861019055509528\n",
      "-----------------\n",
      "чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов . чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов .\n",
      "0.0 0.0 0.035606060606060606 0.006237290050463207\n",
      "-----------------\n",
      "тэд мөн өөрийгөө үнэлэх үпэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг . тэд мөн өөрийгөө үнэлэх үнэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг .\n",
      "0.1 0.014705882352941176 0.04097222222222222 0.006943006075669705\n",
      "-----------------\n",
      "гал хавдрыг засахад уран хараацайн үүрийн шаврыг тйалхдаад тахианы өндөгний цагаанаар зуурч түрхдэг . гал хавдрыг засахад уран хараацайн үүрийн шаврыг талхдаад тахиан өндөгний цагаанаар зуурч түрхдэг .\n",
      "0.14285714285714285 0.020202020202020204 0.04880952380952381 0.007962930239235128\n",
      "-----------------\n",
      "тэр шажин төрийн хавчлаганд орж намаг түр хаяж ч болно . тэр шажин төрийн хавчлаганд орж намайг түр хаяж ч болно .\n",
      "0.09090909090909091 0.017543859649122806 0.051816635745207174 0.00864728233994139\n",
      "-----------------\n",
      "хамгийг мэдэгч тэнгэр дорноос өрнө хүртэлх бүгд газруудыг маань эрхэнд өгснийг та нар санагтун ! хамгийг мэдэгч тэнгэр дорноос өрнө хүртэлх бүгд газруудыг манай эрхэнд өгснийг та нар санагтун !\n",
      "0.06666666666666667 0.03125 0.05280663780663781 0.010154130183945297\n",
      "-----------------\n",
      "хоршооллауд нь эмэгтэйчүүдийг эрчүүдтэй эн тэнцүү болгон чадавхижуулан хөгжүүлэх нэн чухал хэрэгсэл болох боломжтой . хоршооллууд нь эмэгтэйчүүдийг эрчүүдтэй эн тэнцүү болгон чадавхижуулан хөгжүүлэх нэн чухал хэрэгсэл болох боломжтой .\n",
      "0.06666666666666667 0.008547008547008548 0.05367288961038961 0.010053685081636751\n",
      "-----------------\n",
      "хэзээ ч гэсэн аврал эрж болдог . хэзээ ч гэсэн аврал эрж болдог .\n",
      "0.0 0.0 0.050515660809778457 0.009462291841540472\n",
      "-----------------\n",
      "харин 102 , 103 хоёрт хоёуланд нь өөрийн утсаар дуудлага өг ! харин 102 , 103 хоёрт хоёуланд нь өөрийн утсаар дуудлага өг !\n",
      "0.0 0.0 0.04770923520923521 0.00893660896145489\n",
      "-----------------\n",
      "бурхан болсон хүнийг буянаар үд буруу санаа хятадын зэвсгээр үд . бурхан болсон хүнийг буянаар үд буруу санаат хятадыг зэвсгээр үд .\n",
      "0.18181818181818182 0.030303030303030304 0.054767600820232396 0.010061157453116754\n",
      "-----------------\n",
      "энгийн иргэдийг хүчээр нүүлгэн шилжүүлэхийг хориглоно . энгийн иргэдийг хүчээр нүүлгэн шилжүүлэхийг хориглоно .\n",
      "0.0 0.0 0.05202922077922077 0.009558099580460916\n",
      "-----------------\n",
      "энэ бүхнийг боол жил бүр ургацынхаа тодорхой хувиар төлнө . энэ бүхнийг боол жил бүр ургацынхаа тодорхой хувиар төлнө .\n",
      "0.0 0.0 0.04955163883735312 0.009102951981391348\n",
      "-----------------\n",
      "гэвч үүнээс болж би яаж зүдэрч байгааг та харахгүй байна уу ? гэвч үүнээс болж би яаж зүдэрч байгааг та харахгүй байна уу ?\n",
      "0.0 0.0 0.047299291617473434 0.00868918143678265\n",
      "-----------------\n",
      "хүүхдүүд нь ээж ээ , ээж ээ гэж хашгиралдан гэ . хүүхдүүд нь ээж ээ , ээж ээ гэж хашгиралдан гэ .\n",
      "0.0 0.0 0.04524280067758328 0.00831139093953123\n",
      "-----------------\n",
      "яагаад гэвэл тэдний зүрх сэтгэлийг мэргэжил нь идчихдэг юм . яагаад гэвэл тэдний зүрх сэтгэлийг мэргэжил нь идчихдэг юм .\n",
      "0.0 0.0 0.04335768398268398 0.00796508298371743\n",
      "-----------------\n",
      "теннисний хөгжөөн дэмжигчид түүний нэрийг сонсоод л хэний тухай ярьж байгааг шууд мэднэ . теннисний хөгжөөн дэмжигчид түүний нэрийг сонсоод л хэний тухай ярьж байгааг шууд мэднэ .\n",
      "0.0 0.0 0.04162337662337662 0.007646479664368733\n",
      "-----------------\n",
      "хорезм дундад азид оршиж байсан эртний түүхт улс . хорезм дундад азид оршиж байсан эртний түүхт улс .\n",
      "0.0 0.0 0.04002247752247752 0.007352384292662243\n",
      "-----------------\n",
      "буддын шашин энэ тал дээр бусад ихэнх шашнаас тэс ондоо байр суурьтай байдаг . буддын шашин энэ тал дээр бусад ихэнх шашнаас тэс ондоо байр суурьтай байдаг .\n",
      "0.0 0.0 0.038540163540163534 0.0070800737633043825\n",
      "-----------------\n",
      "гэхдээ тэр монгол нутгаар ганцаар аялаагүй . гэхдээ тэр монгол нутгаар ганцаар аялаагүй .\n",
      "0.0 0.0 0.03716372912801484 0.006827213986043511\n",
      "-----------------\n",
      "эмч зээд эхийн нь биед дус гэх юмгүй байна . эмч үзээд эхийн нь биед дус гэх юмгүй байна .\n",
      "0.1 0.022222222222222223 0.03933049708911778 0.007358076339015191\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "test_spell = open(\"train_spell_error.txt\", \"r\")\n",
    "train_clean = open(\"train_clean.txt\", \"r\")\n",
    "\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "train_clean = train_clean.read().split(\"\\n\")\n",
    "\n",
    "WERSUM = 0\n",
    "CERSUM = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_spell)):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    print('-----------------')    \n",
    "    Tw = wer(train_clean[i], temp)\n",
    "    WERSUM += Tw\n",
    "    Tc =  cer(train_clean[i], temp)\n",
    "    CERSUM += Tc\n",
    "    print(temp,train_clean[i])\n",
    "    print(Tw,Tc,WERSUM/(i+1),CERSUM/(i+1))\n",
    "\n",
    "# WER（Word Error Rate）\n",
    "# CER（Character Error Rate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "test_spell = open(\"test_spell_error.txt\", \"r\")\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "OUT = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(test_spell))):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    OUT.append(temp)\n",
    "    print('-----------------')    \n",
    "    print(temp)\n",
    "    print(test_spell[i])\n",
    "    print('-----------------')    \n",
    "\n",
    "with open('test_spell_error_out.txt', 'w') as f:\n",
    "    for item in OUT:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT = []\n",
    "# for i in tqdm(monBigTool.getMASK()):\n",
    "#     sen = []\n",
    "#     labels = []\n",
    "#     for ss in i['sen']:\n",
    "#         if ss == '<>':\n",
    "#             i['word'].pop(0)\n",
    "#             sen.append(i['word'].pop(0))\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [1] * count\n",
    "#         else:\n",
    "#             sen.append(ss)\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [0] * count\n",
    "#     OUT.append({\n",
    "#         'sen':sen,\n",
    "#         'labels':labels\n",
    "#     })\n",
    "# # 保存OUT\n",
    "# import json\n",
    "# with open('sentencePair.json','w') as f:\n",
    "#     json.dump(OUT,f)\n",
    "\n",
    "# # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)\n",
    "#     # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
