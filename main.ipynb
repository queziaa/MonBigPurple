{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T07:40:49.065336Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from MonBigTool import MonBigTool,MASKmodel\n",
    "mASKmodel = MASKmodel()\n",
    "monBigTool = MonBigTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = monBigTool.getMASK()\n",
    "\n",
    "hitT = 0\n",
    "hitF = 0\n",
    "simThreshold = 0.59\n",
    "PScore = 0\n",
    "PError = 0\n",
    "PMiss = 0\n",
    "PTrue = 0\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "\n",
    "\n",
    "\n",
    "COUNT = 0\n",
    "for ii in tqdm(range(len(MASK))):\n",
    "    # ii = 32\n",
    "    i = MASK[ii]\n",
    "    COUNT += 1\n",
    "    # i = random.choice()\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "    #########################################\n",
    "    # 得到词对 信息  ########################\n",
    "    # for g in pairs:\n",
    "    #     if not g[0].isalpha():\n",
    "    #         continue\n",
    "\n",
    "\n",
    "        # te = monBigTool.letterSim(g[0],g[1])\n",
    "        # if te < 0.8:\n",
    "        #     print(g[0],g[1],te)\n",
    "    #########################################\n",
    "\n",
    "\n",
    "    temp = mASKmodel.tomask(fsen,3,False)\n",
    "    SecondaryTreatment = []\n",
    "\n",
    "    for i in range(len(fsen)):\n",
    "        if not fsen[i].isalpha():\n",
    "            continue\n",
    "        if fsen[i] in temp[i] and len(fsen[i]) > 2:\n",
    "            continue\n",
    "        SecondaryTreatment.append(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp2 = mASKmodel.tomask(tsen,3,True)\n",
    "    temp3 = []\n",
    "    for i in SecondaryTreatment:\n",
    "        TTEMP = []\n",
    "        for j in temp2[i]:\n",
    "            if IS_levenshtein_distance_and_operations(fsen[i],j):\n",
    "                TTEMP.append(j)\n",
    "            if len(TTEMP) == 1:\n",
    "                break\n",
    "        temp3.append(TTEMP)\n",
    "            \n",
    "        # print(fsen[i],TTEMP)\n",
    "        if len(TTEMP) == 0:\n",
    "            continue\n",
    "\n",
    "        if not fsen[i].isalpha():\n",
    "            continue\n",
    "        if fsen[i] in TTEMP:\n",
    "            continue\n",
    "        if i not in inode:\n",
    "            PError += 1\n",
    "            print('*'*20)\n",
    "            print(inode)\n",
    "            print('----',' '.join(fsen),'----')\n",
    "            print('----',' '.join(tsen),'----')\n",
    "            print('----',' '.join(sen),'----')\n",
    "            print(fsen[i],temp[i])\n",
    "            print(fsen[i],temp2[i])\n",
    "            print(fsen[i],TTEMP)\n",
    "\n",
    "\n",
    "        elif i in inode:\n",
    "            PTrue += 1\n",
    "            inode.remove(i)\n",
    "            # print('#'*20)\n",
    "\n",
    "    PMiss += len(inode)\n",
    "\n",
    "    if COUNT % 200 == 0:\n",
    "        print('-----------------')\n",
    "        print('PError:',PError)\n",
    "        print('PMiss:',PMiss)\n",
    "        print('PTrue:',PTrue)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(MASK)):\n",
    "    i = MASK[ii]\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "    if fsen[0] == 'нутгийн':\n",
    "        print(ii)\n",
    "IS_levenshtein_distance_and_operations('бас','бус')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross 没有加原始\n",
    "# PError: 123  \n",
    "# PMiss: 61\n",
    "# PTrue: 139\n",
    "\n",
    "\n",
    "# cross 加原始\n",
    "# PError: 123\n",
    "# PMiss: 61\n",
    "# PTrue: 139\n",
    "\n",
    "\n",
    "# 无cross\n",
    "# PError: 313\n",
    "# PMiss: 7\n",
    "# PTrue: 193\n",
    "\n",
    "\n",
    "# cross加原始 排序筛选10\n",
    "# PError: 128\n",
    "# PMiss: 56\n",
    "# PTrue: 144\n",
    "\n",
    "\n",
    "\n",
    "# cross加原始 考虑三重笛卡尔排序筛选10\n",
    "# PError: 82\n",
    "# PMiss: 96\n",
    "# PTrue: 104\n",
    "\n",
    "\n",
    "\n",
    "# cross加原始 考虑三重笛卡尔排序\n",
    "# PError: 77\n",
    "# PMiss: 104\n",
    "# PTrue: 96\n",
    "\n",
    "\n",
    "\n",
    "# cross加原始 考虑三重笛卡尔排序 双重筛选\n",
    "# PError: 64\n",
    "# PMiss: 9\n",
    "# PTrue: 191\n",
    "\n",
    "\n",
    "# cross加原始 考虑三重笛卡尔排序 双重筛选 第一筛选数量减少为5\n",
    "# PError: 68\n",
    "# PMiss: 8\n",
    "# PTrue: 192\n",
    "\n",
    "# cross加原始 考虑三重笛卡尔排序 双重筛选 第一筛选数量减少为3\n",
    "# PError: 68\n",
    "# PMiss: 7\n",
    "# PTrue: 193\n",
    "\n",
    "\n",
    "# cross加原始 考虑四重笛卡尔排序 双重筛选 第一筛选数量减少为3\n",
    "# PError: 29\n",
    "# PMiss: 9\n",
    "# PTrue: 191\n",
    "\n",
    "\n",
    "# cross加原始 考虑四重笛卡尔排序 双重筛选 第一筛选数量减少为3 合并重复权值 短字符全部进入二轮检查 第二论仅保留一个待选\n",
    "# PError: 64\n",
    "# PMiss: 2\n",
    "# PTrue: 198\n",
    "\n",
    "# cross加原始 考虑四重笛卡尔排序 双重筛选 第一筛选数量减少为3 合并重复权值 短字符全部进入二轮检查 第二论 保留*两*个待选\n",
    "# PError: 36\n",
    "# PMiss: 3\n",
    "# PTrue: 197\n",
    "\n",
    "\n",
    "# cross加原始 考虑四重笛卡尔排序 双重筛选 第一筛选数量减少为3 合并重复权值 短字符全部进入二轮检查 第二论保留*两*个待选 第二论待选15->5  新cross权值计算方法 \n",
    "# PError: 26  78留一个\n",
    "# PMiss: 2\n",
    "# PTrue: 198\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-uncased')\n",
    "\n",
    "# 词嵌入\n",
    "input_text = \"тан\"\n",
    "              \n",
    "encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outputs = model(encoded_input)\n",
    "last_hidden_states = outputs[0]\n",
    "# 解码 last_hidden_states\n",
    "\n",
    "# 选择每个位置最可能的 token ID\n",
    "predicted_ids = torch.argmax(last_hidden_states[0], dim=1)\n",
    "\n",
    "\n",
    "# 解码 predicted_ids\n",
    "for i in predicted_ids:\n",
    "    print(tokenizer.decode(i))\n",
    "\n",
    "print(tokenizer.decode(predicted_ids.tolist()))\n",
    "last_hidden_states.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stelist = ['aa1','aa','dsada1sd','']\n",
    "ssd=[1,2,3,4]\n",
    "[ssd[i] for i in range(len(stelist)) if stelist[i].isalpha() or stelist[i]=='']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
