{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "from tqdm import tqdm\n",
    "# from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "# from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import colon,process_list\n",
    "from MonBigTool import MonBigTool,MASKmodel \n",
    "# WordsDict = monBigTool.getWordsDict()\n",
    "MODELNAME = 'tugstugi/bert-large-mongolian-uncased'\n",
    "\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "class fineTuningClass():\n",
    "    def __init__(self,fineT,tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 加载模型和分词器\n",
    "        self.model = BertForTokenClassification.from_pretrained(fineT)\n",
    "        # model = BertForTokenClassification.from_pretrained('./RRRRRRRRRRR')\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "    def predict(self,sen):\n",
    "        sentence = ' '.join(sen)\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs.logits.argmax(-1).tolist()\n",
    "        # print(predictions)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "        sen = sentence.split()\n",
    "        Tindex = 0\n",
    "        SecondaryTreatment = []\n",
    "        for j in range(len(sen)):\n",
    "            aAaaa = tokenizer.tokenize(sen[j])\n",
    "            # print(sen[j],len(aAaaa),predictions[0][Tindex:Tindex+len(aAaaa)])\n",
    "            if all(v == 0 for v in predictions[0][Tindex:Tindex+len(aAaaa)]) == False:\n",
    "                SecondaryTreatment.append(j)\n",
    "            Tindex = Tindex + len(aAaaa)\n",
    "        return SecondaryTreatment\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MonBigPurple():\n",
    "    def __init__(self,fineTuningClass = None,fineFile = None):\n",
    "        self.WINDOW = 3\n",
    "        self.mASKmodel = MASKmodel(MODELNAME)\n",
    "        self.monBigTool = MonBigTool()\n",
    "        self.MASK = self.monBigTool.getMASK()\n",
    "        self.fineTuningClass = fineTuningClass\n",
    "        if self.fineTuningClass != None:\n",
    "            self.fineTuningClasspredict = fineTuningClass(fineFile,self.mASKmodel.tokenizer)\n",
    "\n",
    "        # self.LOSS_PScore = 0\n",
    "        # self.LOSS_PError = 0\n",
    "        # self.LOSS_PMiss = 0\n",
    "        # self.LOSS_PTrue = 0\n",
    "        # self.LOSS_PTrue_errFix = 0\n",
    "        # self.LOSS_PTrue_succFix = 0\n",
    "        # self.LOSS_O_T = 0\n",
    "        # self.LOSS_O_F = 0\n",
    "        # self.LOSS_O_MISS = 0\n",
    "    \n",
    "    def predict(self,fsen):\n",
    "\n",
    "        # inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "        # ########################################\n",
    "        # # 得到词对 信息  ########################\n",
    "        # for g in pairs:\n",
    "        #     if not g[0].isalpha():\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "        #     # if te < 0.8:\n",
    "        #         # print(g[0],g[1],te)\n",
    "        # ########################################\n",
    "\n",
    "        SecondaryTreatment = []\n",
    "\n",
    "\n",
    "        if self.fineTuningClass != None:\n",
    "            SecondaryTreatment = self.fineTuningClasspredict.predict(fsen)\n",
    "        else:\n",
    "            temp = self.mASKmodel.tomask(fsen,self.WINDOW,False)\n",
    "            for i in range(len(fsen)):\n",
    "                if not fsen[i].isalpha():\n",
    "                    continue\n",
    "                if fsen[i] in temp[i] :\n",
    "                    continue\n",
    "                if self.monBigTool.mysterious(fsen[i]):\n",
    "                    continue\n",
    "                # if i in inode:\n",
    "                #     LOSS_O_T += 1\n",
    "                #     inode.remove(i)\n",
    "                # else:\n",
    "                #     LOSS_O_F += 1\n",
    "                SecondaryTreatment.append(i)\n",
    "\n",
    "        \n",
    "        # LOSS_O_MISS += len(inode)\n",
    "        # print('假----',' '.join(fsen),'----')\n",
    "        # print('真----',' '.join(tsen),'----')\n",
    "        # print('MASK----',' '.join(sen),'----')\n",
    "        # print('怀疑----',SecondaryTreatment,'----',inode)\n",
    "        OUT = []\n",
    "        for i in SecondaryTreatment:\n",
    "            candidate,score = self.monBigTool.FuzzySearch(fsen[i])\n",
    "            Target = fsen[i]\n",
    "            TTEMP = self.mASKmodel.hybridPrediction(fsen,i,3,candidate,score)\n",
    "            if len(TTEMP) == 0:\n",
    "                continue\n",
    "            OUT.append({'i':i,'f':Target,'t':TTEMP[0]})\n",
    "        return OUT\n",
    "            # TTEMP = candidate\n",
    "        #     print('目标',Target,':  候选',candidate)\n",
    "        #     print('结果:',TTEMP)\n",
    "\n",
    "\n",
    "        #     if len(TTEMP) == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     if fsen[i] == TTEMP[0]:\n",
    "        #         continue\n",
    "        \n",
    "        #     if i not in inode:\n",
    "        #         LOSS_PError += 1\n",
    "        #         print('*ERR'*20)\n",
    "        #         print(Target,TTEMP)\n",
    "        #     elif i in inode:\n",
    "        #         LOSS_PTrue += 1\n",
    "        #         inode.remove(i)\n",
    "        #         if tsen[i] == TTEMP[0]:\n",
    "        #             print('*SUCFix'*20)\n",
    "        #             LOSS_PTrue_succFix += 1\n",
    "        #         else:\n",
    "        #             print('*ERRFix'*20)\n",
    "        #             LOSS_PTrue_errFix += 1\n",
    "        #         print(Target,TTEMP)\n",
    "        \n",
    "        # LOSS_PMiss += len(inode)\n",
    "        # for i in inode:\n",
    "        #     print('@MISS'*20)\n",
    "        #     print('----',' '.join(fsen),'----')\n",
    "        #     print('----',' '.join(tsen),'----')\n",
    "        #     print('----',' '.join(sen),'----')\n",
    "        #     print(fsen[i])\n",
    "        #     print(fsen[i])\n",
    "\n",
    "        # if COUNT % 50 == 0:\n",
    "        # # print('-----------------')\n",
    "        #     print('O_T:',LOSS_O_T)\n",
    "        #     print('O_F:',LOSS_O_F)\n",
    "        #     print('O_MISS:',LOSS_O_MISS)\n",
    "        # print('PError:',LOSS_PError)\n",
    "        # print('PMiss:',LOSS_PMiss)|\n",
    "        # print('PTrue:',LOSS_PTrue)\n",
    "        # print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "        # print('PTrue_errFix:',LOSS_PTrue_errFix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "# te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "te = MonBigPurple()\n",
    "test_spell = open(\"train_spell_error.txt\", \"r\")\n",
    "train_clean = open(\"train_clean.txt\", \"r\")\n",
    "\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "train_clean = train_clean.read().split(\"\\n\")\n",
    "\n",
    "WERSUM = 0\n",
    "CERSUM = 0\n",
    "\n",
    "wordCount = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_spell)):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    WORDLEN = len(temp)\n",
    "    wordCount += WORDLEN\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    print('-----------------')    \n",
    "    Tw = wer(train_clean[i], temp)\n",
    "    WERSUM += Tw*WORDLEN\n",
    "    Tc =  cer(train_clean[i], temp)\n",
    "    CERSUM += Tc*WORDLEN\n",
    "\n",
    "    print(temp,train_clean[i])\n",
    "    print(Tw/WORDLEN,Tc/WORDLEN,WERSUM/wordCount,CERSUM/wordCount)\n",
    "\n",
    "# WER（Word Error Rate）\n",
    "# CER（Character Error Rate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "# test_spell = open(\"test_spell_error.txt\", \"r\")\n",
    "# test_spell = test_spell.read().split(\"\\n\")\n",
    "# OUT = []\n",
    "\n",
    "\n",
    "# for i in tqdm(range(len(test_spell))):\n",
    "#     temp = process_list(test_spell[i])\n",
    "#     temp = colon(temp)\n",
    "#     predict = te.predict(temp)\n",
    "#     for j in predict:\n",
    "#         t = j['t']\n",
    "#         ii = j['i']\n",
    "\n",
    "#         temp[ii] = t\n",
    "#     temp = ' '.join(temp)\n",
    "#     temp = temp.replace(' : ',': ')\n",
    "#     OUT.append(temp)\n",
    "#     print('-----------------')    \n",
    "#     print(temp)\n",
    "#     print(test_spell[i])\n",
    "#     print('-----------------')    \n",
    "\n",
    "# with open('test_spell_error_out.txt', 'w') as f:\n",
    "#     for item in OUT:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT = []\n",
    "# for i in tqdm(monBigTool.getMASK()):\n",
    "#     sen = []\n",
    "#     labels = []\n",
    "#     for ss in i['sen']:\n",
    "#         if ss == '<>':\n",
    "#             i['word'].pop(0)\n",
    "#             sen.append(i['word'].pop(0))\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [1] * count\n",
    "#         else:\n",
    "#             sen.append(ss)\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [0] * count\n",
    "#     OUT.append({\n",
    "#         'sen':sen,\n",
    "#         'labels':labels\n",
    "#     })\n",
    "# # 保存OUT\n",
    "# import json\n",
    "# with open('sentencePair.json','w') as f:\n",
    "#     json.dump(OUT,f)\n",
    "\n",
    "# # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)\n",
    "#     # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
