{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T07:40:49.065336Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import MonBigTool,MASKmodel\n",
    "\n",
    "mASKmodel = MASKmodel()\n",
    "monBigTool = MonBigTool()\n",
    "WordsDict = monBigTool.getWordsDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = monBigTool.getMASK()\n",
    "WINDOW = 3\n",
    "\n",
    "LOSS_PScore = 0\n",
    "LOSS_PError = 0\n",
    "LOSS_PMiss = 0\n",
    "LOSS_PTrue = 0\n",
    "LOSS_PTrue_errFix = 0\n",
    "LOSS_PTrue_succFix = 0\n",
    "LOSS_O_T = 0\n",
    "LOSS_O_F = 0\n",
    "LOSS_O_MISS = 0\n",
    "\n",
    "\n",
    "COUNT = 0\n",
    "LENMASK = len(MASK)\n",
    "for ii in tqdm(range(LENMASK)):\n",
    "    ii = random.choice(range(LENMASK))\n",
    "    i = MASK[ii]\n",
    "    COUNT += 1\n",
    "    # i = random.choice()\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "    # ########################################\n",
    "    # # 得到词对 信息  ########################\n",
    "    # for g in pairs:\n",
    "    #     if not g[0].isalpha():\n",
    "    #         continue\n",
    "\n",
    "\n",
    "    #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "    #     # if te < 0.8:\n",
    "    #         # print(g[0],g[1],te)\n",
    "    # ########################################\n",
    "\n",
    "\n",
    "    temp = mASKmodel.tomask(fsen,WINDOW,False)\n",
    "    SecondaryTreatment = []\n",
    "    for i in range(len(fsen)):\n",
    "        if not fsen[i].isalpha():\n",
    "            continue\n",
    "        if fsen[i] in temp[i] :\n",
    "            continue\n",
    "        if monBigTool.mysterious(fsen[i]):\n",
    "            continue\n",
    "\n",
    "        if i in inode:\n",
    "            LOSS_O_T += 1\n",
    "            inode.remove(i)\n",
    "        else:\n",
    "            LOSS_O_F += 1\n",
    "        SecondaryTreatment.append(i)\n",
    "    LOSS_O_MISS += len(inode)\n",
    "    # print('假----',' '.join(fsen),'----')\n",
    "    # print('真----',' '.join(tsen),'----')\n",
    "    # print('MASK----',' '.join(sen),'----')\n",
    "    # print('怀疑----',SecondaryTreatment,'----',inode)\n",
    "\n",
    "\n",
    "    # for i in SecondaryTreatment:\n",
    "    #     Alternate = []\n",
    "    #     candidate,score = monBigTool.FuzzySearch(fsen[i])\n",
    "    #     Target = fsen[i]\n",
    "\n",
    "    #     TTEMP = mASKmodel.hybridPrediction(fsen,i,3,candidate,score)\n",
    "    #     # TTEMP = candidate\n",
    "    #     print('目标',Target,':  候选',candidate)\n",
    "    #     print('结果:',TTEMP)\n",
    "\n",
    "\n",
    "    #     if len(TTEMP) == 0:\n",
    "    #         continue\n",
    "\n",
    "    #     if fsen[i] == TTEMP[0]:\n",
    "    #         continue\n",
    "    \n",
    "    #     if i not in inode:\n",
    "    #         LOSS_PError += 1\n",
    "    #         print('*ERR'*20)\n",
    "    #         print(Target,TTEMP)\n",
    "    #     elif i in inode:\n",
    "    #         LOSS_PTrue += 1\n",
    "    #         inode.remove(i)\n",
    "    #         if tsen[i] == TTEMP[0]:\n",
    "    #             print('*SUCFix'*20)\n",
    "    #             LOSS_PTrue_succFix += 1\n",
    "    #         else:\n",
    "    #             print('*ERRFix'*20)\n",
    "    #             LOSS_PTrue_errFix += 1\n",
    "    #         print(Target,TTEMP)\n",
    "    \n",
    "    # LOSS_PMiss += len(inode)\n",
    "    # for i in inode:\n",
    "    #     print('@MISS'*20)\n",
    "    #     print('----',' '.join(fsen),'----')\n",
    "    #     print('----',' '.join(tsen),'----')\n",
    "    #     print('----',' '.join(sen),'----')\n",
    "    #     print(fsen[i])\n",
    "    #     print(fsen[i])\n",
    "\n",
    "    if COUNT % 50 == 0:\n",
    "    # print('-----------------')\n",
    "        print('O_T:',LOSS_O_T)\n",
    "        print('O_F:',LOSS_O_F)\n",
    "        print('O_MISS:',LOSS_O_MISS)\n",
    "    # print('PError:',LOSS_PError)\n",
    "    # print('PMiss:',LOSS_PMiss)\n",
    "    # print('PTrue:',LOSS_PTrue)\n",
    "    # print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "    # print('PTrue_errFix:',LOSS_PTrue_errFix)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PError: 61\n",
    "# PMiss: 37\n",
    "# PTrue: 161\n",
    "# PTrue_succFix: 140\n",
    "# PTrue_errFix: 21\n",
    "#   0%|          | 101/43061 [11:21<80:33:02,  6.75s/it]\n",
    "\n",
    "\n",
    "# PError: 62\n",
    "# PMiss: 27\n",
    "# PTrue: 150\n",
    "# PTrue_succFix: 135\n",
    "# PTrue_errFix: 15\n",
    "#   0%|          | 101/43061 [14:54<143:19:06, 12.01s/it]  添加词频权重\n",
    "\n",
    "\n",
    "\n",
    "# PError: 67\n",
    "# PMiss: 21\n",
    "# PTrue: 146\n",
    "# PTrue_succFix: 124\n",
    "# PTrue_errFix: 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(MASK)):\n",
    "    i = MASK[ii]\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "    if fsen[0] == 'энэ':\n",
    "        print(ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-uncased')\n",
    "\n",
    "# 词嵌入\n",
    "input_text = \"хэрэв зориудаар алсан оол тэр хэмжээний малаар торгууль төлж цайруулмуй\"\n",
    "              \n",
    "encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outputs = model(encoded_input)\n",
    "\n",
    "\n",
    "last_hidden_states = outputs[0]\n",
    "# 解码 last_hidden_states\n",
    "\n",
    "# 选择每个位置最可能的 token ID\n",
    "predicted_ids = torch.argmax(last_hidden_states[0], dim=1)\n",
    "\n",
    "print(input_text)\n",
    "print(tokenizer.decode(predicted_ids.tolist()))\n",
    "\n",
    "\n",
    "# 真---- хэрэв зориудаар алсан бол тэр хэмжээний малаар торгууль төлж цайруулмуй . ----\n",
    "# MASK---- хэрэв зориудаар алсан [MASK] тэр хэмжээний малаар торгууль төлж [MASK] . ----\n",
    "# 怀疑---- [3, 6, 9] ---- [3, 9]\n",
    "# обл :  : ['сбл', 'ойл', 'обя', 'обг', 'убл', 'ол', 'оол', 'орл', 'оёл', 'об', 'бл', 'оби', 'оба', 'онл', 'нобл', 'обь', 'обх', 'дбл', 'ббл', 'обс', 'олл', 'оул', 'обт', 'обл', 'обе', 'эбл', 'обо', 'облж', 'шбл', 'обол', 'обу', 'осл', 'гобл', 'оил', 'общ', 'охл', 'хобл', 'обла', 'собл', 'обб', 'оел', 'объ', 'робл', 'добл', 'отл', 'нбл', 'лбл', 'обш', 'мбл']\n",
    "# 结果: ['мбл', 'нбл', 'мол', 'бол', 'нол']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-uncased',output_hidden_states=True)\n",
    "\n",
    "# 词嵌入\n",
    "input_text = \"хэрэв зориудаар алсан оол тэр хэмжээний малаар торгууль төлж цайруулмуй\"\n",
    "              \n",
    "encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outputs = model(encoded_input)\n",
    "\n",
    "outputs.logits.shape\n",
    "len(outputs.hidden_states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 加载预训练模型，指定要进行分类的类别数量\n",
    "model = BertForTokenClassification.from_pretrained('tugstugi/bert-large-mongolian-uncased', num_labels=2)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出目录\n",
    "    num_train_epochs=3,              # 总的训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=64,   # 每个设备的评估批次大小\n",
    "    warmup_steps=500,                # 预热步数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    ")\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 要训练的模型\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=train_dataset,         # 训练数据集\n",
    "    eval_dataset=test_dataset            # 评估数据集\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModelForMaskedLM\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "    # 读取\n",
    "\n",
    "\n",
    "class SpellingErrorDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # 对文本进行编码\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # 将标签对齐到编码的长度\n",
    "        labels = labels + [0] * (self.max_len - len(labels))\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-uncased')\n",
    "# 创建分词器\n",
    "\n",
    "# 创建数据集\n",
    "texts = [\"зэрэг, хамгаалагчид!\", \"Goodbye, world!\"]\n",
    "labels = [[0, 0, 1, 0], [0, 0, 0, 1]]  # 假设我们有两个类别，0表示正确，1表示拼写错误\n",
    "dataset = SpellingErrorDataset(texts, labels, tokenizer, max_len=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 打印\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = []\n",
    "for i in tqdm(monBigTool.getMASK()):\n",
    "    sen = []\n",
    "    labels = []\n",
    "    for ss in i['sen']:\n",
    "        if ss == '<>':\n",
    "            i['word'].pop(0)\n",
    "            sen.append(i['word'].pop(0))\n",
    "            count = sen[-1]\n",
    "            count = len(tokenizer(count)['input_ids'])\n",
    "            labels += [1] * count\n",
    "        else:\n",
    "            sen.append(ss)\n",
    "            count = sen[-1]\n",
    "            count = len(tokenizer(count)['input_ids'])\n",
    "            labels += [0] * count\n",
    "    OUT.append({\n",
    "        'sen':sen,\n",
    "        'labels':labels\n",
    "    })\n",
    "# 保存OUT\n",
    "import json\n",
    "with open('sentencePair.json','w') as f:\n",
    "    json.dump(OUT,f)\n",
    "\n",
    "# 读取\n",
    "import json\n",
    "with open('sentencePair.json','r') as f:\n",
    "    OUT = json.load(f)\n",
    "    # 读取\n",
    "import json\n",
    "with open('sentencePair.json','r') as f:\n",
    "    OUT = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./RRRRRRRRRRR were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "цаашлаад 1 [0]\n",
      "бид 1 [0]\n",
      "чуулганыхаа 2 [0, 0]\n",
      "библийн 3 [0, 1, 1]\n",
      "сургалт 1 [1]\n",
      ", 2 [0, 0]\n",
      "илгээлт 1 [0]\n",
      "гээд 1 [0]\n",
      "юу 1 [0]\n",
      "эсэхийг 1 [0]\n",
      "ярихсан 2 [0, 1]\n",
      "билээ 1 [0]\n",
      ". 2 [0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "MODELNAME = 'tugstugi/bert-large-mongolian-uncased'\n",
    "# 加载模型和分词器\n",
    "model = BertForTokenClassification.from_pretrained('./RRRRRRRRRRR')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "# 输入你的句子\n",
    "sentence = \"цаашлаад бид чуулганыхаа библийн сургалт , илгээлт гээд юу эсэхийг ярихсан билээ .\"\n",
    "\n",
    "\n",
    "# 对句子进行编码\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型的预测结果\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 获取预测的标签\n",
    "predictions = outputs.logits.argmax(-1).tolist()\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "\n",
    "sen = sentence.split()\n",
    "Tindex = 0\n",
    "for j in range(len(sen)):\n",
    "    aAaaa = tokenizer.tokenize(sen[j])\n",
    "    print(sen[j],len(aAaaa),predictions[0][Tindex:Tindex+len(aAaaa)])\n",
    "    Tindex = Tindex + len(aAaaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
