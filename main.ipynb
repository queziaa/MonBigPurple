{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T07:40:49.065336Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "from MonBigTool import MonBigTool,MASKmodel\n",
    "\n",
    "mASKmodel = MASKmodel()\n",
    "monBigTool = MonBigTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = monBigTool.getMASK()\n",
    "WINDOW = 3\n",
    "\n",
    "LOSS_PScore = 0\n",
    "LOSS_PError = 0\n",
    "LOSS_PMiss = 0\n",
    "LOSS_PTrue = 0\n",
    "LOSS_PTrue_errFix = 0\n",
    "LOSS_PTrue_succFix = 0\n",
    "\n",
    "\n",
    "\n",
    "COUNT = 0\n",
    "LENMASK = len(MASK)\n",
    "for ii in tqdm(range(LENMASK)):\n",
    "    ii = random.choice(range(LENMASK))\n",
    "    i = MASK[ii]\n",
    "    COUNT += 1\n",
    "    # i = random.choice()\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "    # ########################################\n",
    "    # # 得到词对 信息  ########################\n",
    "    # for g in pairs:\n",
    "    #     if not g[0].isalpha():\n",
    "    #         continue\n",
    "\n",
    "\n",
    "    #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "    #     # if te < 0.8:\n",
    "    #         # print(g[0],g[1],te)\n",
    "    # ########################################\n",
    "\n",
    "\n",
    "    temp = mASKmodel.tomask(fsen,WINDOW,False)\n",
    "    SecondaryTreatment = []\n",
    "\n",
    "    for i in range(len(fsen)):\n",
    "        if not fsen[i].isalpha():\n",
    "            continue\n",
    "        if fsen[i] in temp[i] and len(fsen[i]) > 2:\n",
    "            continue\n",
    "        SecondaryTreatment.append(i)\n",
    "\n",
    "\n",
    "    temp2 = mASKmodel.tomask(sen,WINDOW,True)\n",
    "    temp3 = []\n",
    "    for i in SecondaryTreatment:\n",
    "        TTEMP = []\n",
    "        for j in temp2[i]:\n",
    "            if IS_levenshtein_distance_and_operations(fsen[i],j):\n",
    "                TTEMP.append(j)\n",
    "            if len(TTEMP) == WINDOW+3:\n",
    "                break\n",
    "        temp3.append(TTEMP)\n",
    "            \n",
    "        if len(TTEMP) == 0:\n",
    "            continue\n",
    "\n",
    "        if not fsen[i].isalpha():\n",
    "            continue\n",
    "        if fsen[i] in TTEMP:\n",
    "            continue\n",
    "        if i not in inode:\n",
    "            LOSS_PError += 1\n",
    "            # print('*'*20)\n",
    "            # print('----',' '.join(fsen),'----')\n",
    "            # print('----',' '.join(tsen),'----')\n",
    "            # print('----',' '.join(sen),'----')\n",
    "            # print(fsen[i],temp[i])\n",
    "            # print(fsen[i],temp2[i])\n",
    "            # print(fsen[i],TTEMP)\n",
    "        elif i in inode:\n",
    "            LOSS_PTrue += 1\n",
    "            inode.remove(i)\n",
    "            if tsen[i] == TTEMP[0]:\n",
    "                LOSS_PTrue_succFix += 1\n",
    "            else:\n",
    "                LOSS_PTrue_errFix += 1\n",
    "    LOSS_PMiss += len(inode)\n",
    "    for i in inode:\n",
    "        print('@'*20)\n",
    "        print('----',' '.join(fsen),'----')\n",
    "        print('----',' '.join(tsen),'----')\n",
    "        print('----',' '.join(sen),'----')\n",
    "        print(fsen[i],temp[i])\n",
    "        print(fsen[i],temp2[i])\n",
    "        print(fsen[i],TTEMP)\n",
    "\n",
    "    if COUNT % 50 == 0:\n",
    "        print('-----------------')\n",
    "        print('PError:',LOSS_PError)\n",
    "        print('PMiss:',LOSS_PMiss)\n",
    "        print('PTrue:',LOSS_PTrue)\n",
    "        print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "        print('PTrue_errFix:',LOSS_PTrue_errFix)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PError: 9\n",
    "# PMiss: 104\n",
    "# PTrue: 86\n",
    "# PTrue_succFix: 78\n",
    "# PTrue_errFix: 8\n",
    "\n",
    "# PError: 5\n",
    "# PMiss: 107\n",
    "# PTrue: 82\n",
    "# PTrue_succFix: 78\n",
    "# PTrue_errFix: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1352969/1352969 [00:05<00:00, 244016.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['эхэлжээ',\n",
       " 'бэхэлжээ',\n",
       " 'мэхэлжээ',\n",
       " 'эхэлжээв',\n",
       " 'эрхэлжээ',\n",
       " 'эхэлжжээ',\n",
       " 'эхэлэжээ',\n",
       " 'эмхэлжээ',\n",
       " 'эхлэлжээ',\n",
       " 'эхэлжэээ',\n",
       " 'эхэлжэнэ',\n",
       " 'эхэлэщнэ',\n",
       " 'цэхэлжээ',\n",
       " 'эхэлждээ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]+\n",
    "a + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(MASK)):\n",
    "    i = MASK[ii]\n",
    "    inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "    if fsen[0] == 'энэ':\n",
    "        print(ii)\n",
    "# IS_levenshtein_distance_and_operations('өрнө', 'төрнө')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained('tugstugi/bert-large-mongolian-uncased')\n",
    "\n",
    "# 词嵌入\n",
    "input_text = \"тан\"\n",
    "              \n",
    "encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outputs = model(encoded_input)\n",
    "last_hidden_states = outputs[0]\n",
    "# 解码 last_hidden_states\n",
    "\n",
    "# 选择每个位置最可能的 token ID\n",
    "predicted_ids = torch.argmax(last_hidden_states[0], dim=1)\n",
    "\n",
    "\n",
    "# 解码 predicted_ids\n",
    "for i in predicted_ids:\n",
    "    print(tokenizer.decode(i))\n",
    "\n",
    "print(tokenizer.decode(predicted_ids.tolist()))\n",
    "last_hidden_states.shape\n",
    "stelist = ['aa1','aa','dsada1sd','']\n",
    "ssd=[1,2,3,4]\n",
    "[ssd[i] for i in range(len(stelist)) if stelist[i].isalpha() or stelist[i]=='']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
