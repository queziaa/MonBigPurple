{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "from tqdm import tqdm\n",
    "# from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "# from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import colon,process_list\n",
    "from MonBigTool import MonBigTool,MASKmodel \n",
    "# WordsDict = monBigTool.getWordsDict()\n",
    "MODELNAME = 'tugstugi/bert-large-mongolian-uncased'\n",
    "\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "class fineTuningClass():\n",
    "    def __init__(self,fineT,tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 加载模型和分词器\n",
    "        self.model = BertForTokenClassification.from_pretrained(fineT)\n",
    "        # model = BertForTokenClassification.from_pretrained('./RRRRRRRRRRR')\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "\n",
    "    def predict(self,sen):\n",
    "        sentence = ' '.join(sen)\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs.logits.argmax(-1).tolist()\n",
    "        # print(predictions)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODELNAME, use_fast=False)\n",
    "        sen = sentence.split()\n",
    "        Tindex = 0\n",
    "        SecondaryTreatment = []\n",
    "        for j in range(len(sen)):\n",
    "            aAaaa = tokenizer.tokenize(sen[j])\n",
    "            # print(sen[j],len(aAaaa),predictions[0][Tindex:Tindex+len(aAaaa)])\n",
    "            if all(v == 0 for v in predictions[0][Tindex:Tindex+len(aAaaa)]) == False:\n",
    "                SecondaryTreatment.append(j)\n",
    "            Tindex = Tindex + len(aAaaa)\n",
    "        return SecondaryTreatment\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MonBigPurple():\n",
    "    def __init__(self,fineTuningClass = None,fineFile = None):\n",
    "        self.WINDOW = 3\n",
    "        self.mASKmodel = MASKmodel(MODELNAME)\n",
    "        self.monBigTool = MonBigTool()\n",
    "        self.MASK = self.monBigTool.getMASK()\n",
    "        self.fineTuningClass = fineTuningClass\n",
    "        if self.fineTuningClass != None:\n",
    "            self.fineTuningClasspredict = fineTuningClass(fineFile,self.mASKmodel.tokenizer)\n",
    "\n",
    "        # self.LOSS_PScore = 0\n",
    "        # self.LOSS_PError = 0\n",
    "        # self.LOSS_PMiss = 0\n",
    "        # self.LOSS_PTrue = 0\n",
    "        # self.LOSS_PTrue_errFix = 0\n",
    "        # self.LOSS_PTrue_succFix = 0\n",
    "        # self.LOSS_O_T = 0\n",
    "        # self.LOSS_O_F = 0\n",
    "        # self.LOSS_O_MISS = 0\n",
    "    \n",
    "    def predict(self,fsen):\n",
    "\n",
    "        # inode,tsen,fsen,sen,freq,pairs = monBigTool.Decode(i)\n",
    "\n",
    "        # ########################################\n",
    "        # # 得到词对 信息  ########################\n",
    "        # for g in pairs:\n",
    "        #     if not g[0].isalpha():\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        #     # te = monBigTool.letterSim(g[0],g[1])\n",
    "        #     # if te < 0.8:\n",
    "        #         # print(g[0],g[1],te)\n",
    "        # ########################################\n",
    "\n",
    "        SecondaryTreatment = []\n",
    "\n",
    "\n",
    "        if self.fineTuningClass != None:\n",
    "            SecondaryTreatment = self.fineTuningClasspredict.predict(fsen)\n",
    "        else:\n",
    "            temp = self.mASKmodel.tomask(fsen,self.WINDOW,False)\n",
    "            for i in range(len(fsen)):\n",
    "                if not fsen[i].isalpha():\n",
    "                    continue\n",
    "                if fsen[i] in temp[i] :\n",
    "                    continue\n",
    "                if self.monBigTool.mysterious(fsen[i]):\n",
    "                    continue\n",
    "                # if i in inode:\n",
    "                #     LOSS_O_T += 1\n",
    "                #     inode.remove(i)\n",
    "                # else:\n",
    "                #     LOSS_O_F += 1\n",
    "                SecondaryTreatment.append(i)\n",
    "\n",
    "        \n",
    "        # LOSS_O_MISS += len(inode)\n",
    "        # print('假----',' '.join(fsen),'----')\n",
    "        # print('真----',' '.join(tsen),'----')\n",
    "        # print('MASK----',' '.join(sen),'----')\n",
    "        # print('怀疑----',SecondaryTreatment,'----',inode)\n",
    "        OUT = []\n",
    "        for i in SecondaryTreatment:\n",
    "            candidate,score = self.monBigTool.FuzzySearch(fsen[i])\n",
    "            Target = fsen[i]\n",
    "            TTEMP = self.mASKmodel.hybridPrediction(fsen,i,3,candidate,score)\n",
    "            if len(TTEMP) == 0:\n",
    "                continue\n",
    "            OUT.append({'i':i,'f':Target,'t':TTEMP[0]})\n",
    "        return OUT\n",
    "            # TTEMP = candidate\n",
    "        #     print('目标',Target,':  候选',candidate)\n",
    "        #     print('结果:',TTEMP)\n",
    "\n",
    "\n",
    "        #     if len(TTEMP) == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     if fsen[i] == TTEMP[0]:\n",
    "        #         continue\n",
    "        \n",
    "        #     if i not in inode:\n",
    "        #         LOSS_PError += 1\n",
    "        #         print('*ERR'*20)\n",
    "        #         print(Target,TTEMP)\n",
    "        #     elif i in inode:\n",
    "        #         LOSS_PTrue += 1\n",
    "        #         inode.remove(i)\n",
    "        #         if tsen[i] == TTEMP[0]:\n",
    "        #             print('*SUCFix'*20)\n",
    "        #             LOSS_PTrue_succFix += 1\n",
    "        #         else:\n",
    "        #             print('*ERRFix'*20)\n",
    "        #             LOSS_PTrue_errFix += 1\n",
    "        #         print(Target,TTEMP)\n",
    "        \n",
    "        # LOSS_PMiss += len(inode)\n",
    "        # for i in inode:\n",
    "        #     print('@MISS'*20)\n",
    "        #     print('----',' '.join(fsen),'----')\n",
    "        #     print('----',' '.join(tsen),'----')\n",
    "        #     print('----',' '.join(sen),'----')\n",
    "        #     print(fsen[i])\n",
    "        #     print(fsen[i])\n",
    "\n",
    "        # if COUNT % 50 == 0:\n",
    "        # # print('-----------------')\n",
    "        #     print('O_T:',LOSS_O_T)\n",
    "        #     print('O_F:',LOSS_O_F)\n",
    "        #     print('O_MISS:',LOSS_O_MISS)\n",
    "        # print('PError:',LOSS_PError)\n",
    "        # print('PMiss:',LOSS_PMiss)\n",
    "        # print('PTrue:',LOSS_PTrue)\n",
    "        # print('PTrue_succFix:',LOSS_PTrue_succFix)\n",
    "        # print('PTrue_errFix:',LOSS_PTrue_errFix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "test_spell = open(\"train_spell_error.txt\", \"r\")\n",
    "train_clean = open(\"train_clean.txt\", \"r\")\n",
    "\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "train_clean = train_clean.read().split(\"\\n\")\n",
    "\n",
    "WERSUM = 0\n",
    "CERSUM = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_spell)):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    print('-----------------')    \n",
    "    Tw = wer(train_clean[i], temp)\n",
    "    WERSUM += Tw\n",
    "    Tc =  cer(train_clean[i], temp)\n",
    "    CERSUM += Tc\n",
    "    print(temp,train_clean[i])\n",
    "    print(Tw,Tc,WERSUM/(i+1),CERSUM/(i+1))\n",
    "\n",
    "# WER（Word Error Rate）\n",
    "# CER（Character Error Rate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  warnings.warn(\n",
    "Some weights of the model checkpoint at ./RRRRRRRRRRR were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
    "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "-----------------\n",
    "хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ . хүүхэд буруу зүйл хийсэн даруйд хажууд нь бэлэн байхыг хүснэ .\n",
    "-----------------\n",
    "зэрэг хамгаалагчид нь бас өөр хоорондоо маргалдаан явуулж сурсан эрдмээ олны дунд илэрхийлдэг . зэрэг хамгаалагчид нь бас өөр хоорондоо маргалдаан явуулж сурсан эрдмээ олны дунд илэрхийлдэг .\n",
    "-----------------\n",
    "хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө ангийн шинж чанартай байдаг . хүний сайхан бол харьцангуй чанар бөгөөд ямагт үндэсний арьс өнгө ангийн шинж чанартай байдаг .\n",
    "-----------------\n",
    "монгол эмэнд үндсийг зүйл хамгийн арвин байдаг . монгол эмэнд үндсийн зүйл хамгийн арвин байдаг .\n",
    "-----------------\n",
    "нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг . нохойн эрдэнэ бол ходоод муудаж зангирах өвчнийг засдаг .\n",
    "-----------------\n",
    "харин одоо миний нүд таныг хардаг . харин одоо миний нүд таныг хардаг .\n",
    "-----------------\n",
    "эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ . эвлэрнэ гэдэг бууж өгч байгаа хэрэг биш ээ .\n",
    "-----------------\n",
    "хаан та морио далайн дундах цээл арал дээр дасгав . хаан та морио далайн дундах цээж арал дээр дасгав .\n",
    "-----------------\n",
    "энэ бослогыг бүрэн дарснаар засгийн газар уяа хүлээснээсээ салав . энэ бослогыг бүрэн дарснаар засгийн газар уяа хүлээснээсээ салав .\n",
    "-----------------\n",
    "эдгээр н бизнес үргэлж хэрэгжсээр ирсэн бөгөөд цаашид ч тэгэх болно . эдгээр нь бизнест үргэлж хэрэгжсээр ирсэн бөгөөд цаашид ч тэгэх болно .\n",
    "-----------------\n",
    "чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов . чингис хаан бас өгүүлэгтүн хэмээн зарлиг болов .\n",
    "-----------------\n",
    "тэд мөн өөрийгөө үнэлэх үпэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг . тэд мөн өөрийгөө үнэлэх үнэлэмжээ эргүүлэн шаардах хэрэгтэй байдаг .\n",
    "-----------------\n",
    "гал хавдрыг засахад уран хараацайн үүрийн шаврыг тйалхдаад тахианы өндөгний цагаанаар зуурч түрхдэг . гал хавдрыг засахад уран хараацайн үүрийн шаврыг талхдаад тахиан өндөгний цагаанаар зуурч түрхдэг .\n",
    "-----------------\n",
    "тэр шажин төрийн хавчлаганд орж намаг түр хаяж ч болно . тэр шажин төрийн хавчлаганд орж намайг түр хаяж ч болно .\n",
    "-----------------\n",
    "хамгийг мэдэгч тэнгэр дорноос өрнө хүртэлх бүгд газруудыг маань эрхэнд өгснийг та нар санагтун ! хамгийг мэдэгч тэнгэр дорноос өрнө хүртэлх бүгд газруудыг манай эрхэнд өгснийг та нар санагтун !\n",
    "-----------------\n",
    "хоршооллауд нь эмэгтэйчүүдийг эрчүүдтэй эн тэнцүү болгон чадавхижуулан хөгжүүлэх нэн чухал хэрэгсэл болох боломжтой . хоршооллууд нь эмэгтэйчүүдийг эрчүүдтэй эн тэнцүү болгон чадавхижуулан хөгжүүлэх нэн чухал хэрэгсэл болох боломжтой .\n",
    "-----------------\n",
    "хэзээ ч гэсэн аврал эрж болдог . хэзээ ч гэсэн аврал эрж болдог .\n",
    "-----------------\n",
    "харин 102 , 103 хоёрт хоёуланд нь өөрийн утсаар дуудлага өг ! харин 102 , 103 хоёрт хоёуланд нь өөрийн утсаар дуудлага өг !\n",
    "-----------------\n",
    "бурхан болсон хүнийг буянаар үд буруу санаа хятадын зэвсгээр үд . бурхан болсон хүнийг буянаар үд буруу санаат хятадыг зэвсгээр үд .\n",
    "-----------------\n",
    "энгийн иргэдийг хүчээр нүүлгэн шилжүүлэхийг хориглоно . энгийн иргэдийг хүчээр нүүлгэн шилжүүлэхийг хориглоно .\n",
    "-----------------\n",
    "энэ бүхнийг боол жил бүр ургацынхаа тодорхой хувиар төлнө . энэ бүхнийг боол жил бүр ургацынхаа тодорхой хувиар төлнө .\n",
    "-----------------\n",
    "гэвч үүнээс болж би яаж зүдэрч байгааг та харахгүй байна уу ? гэвч үүнээс болж би яаж зүдэрч байгааг та харахгүй байна уу ?\n",
    "-----------------\n",
    "хүүхдүүд нь ээж ээ , ээж ээ гэж хашгиралдан гэ . хүүхдүүд нь ээж ээ , ээж ээ гэж хашгиралдан гэ .\n",
    "-----------------\n",
    "яагаад гэвэл тэдний зүрх сэтгэлийг мэргэжил нь идчихдэг юм . яагаад гэвэл тэдний зүрх сэтгэлийг мэргэжил нь идчихдэг юм .\n",
    "-----------------\n",
    "теннисний хөгжөөн дэмжигчид түүний нэрийг сонсоод л хэний тухай ярьж байгааг шууд мэднэ . теннисний хөгжөөн дэмжигчид түүний нэрийг сонсоод л хэний тухай ярьж байгааг шууд мэднэ .\n",
    "-----------------\n",
    "хорезм дундад азид оршиж байсан эртний түүхт улс . хорезм дундад азид оршиж байсан эртний түүхт улс .\n",
    "-----------------\n",
    "буддын шашин энэ тал дээр бусад ихэнх шашнаас тэс ондоо байр суурьтай байдаг . буддын шашин энэ тал дээр бусад ихэнх шашнаас тэс ондоо байр суурьтай байдаг .\n",
    "-----------------\n",
    "гэхдээ тэр монгол нутгаар ганцаар аялаагүй . гэхдээ тэр монгол нутгаар ганцаар аялаагүй .\n",
    "-----------------\n",
    "эмч зээд эхийн нь биед дус гэх юмгүй байна . эмч үзээд эхийн нь биед дус гэх юмгүй байна .\n",
    "-----------------\n",
    "хүүхдүүд чинь л чамайг хэдэн жил амьд авч явах хувь учиртай юм байж . хүүхдүүд чинь л чамайг хэдэн жил амьд авч явах хувь учиртай юм байж .\n",
    "-----------------\n",
    "тэд нар дээдсийн бичгийн үгийг сэлгэн хольж , утгыг гуйвуулжухуй . тэд нар дээдийн бичгийн үгийг сэлгэн хольж , утгыг гуйвуулжухуй .\n",
    "-----------------\n",
    "тэнгэрийн тэтгэсэний 17 р онд улсад туслагч гүн залкгамжилжээ . тэнгэрийн тэтгэсэний 17 р онд улсад туслагч гүн залгамжилжээ .\n",
    "-----------------\n",
    "эгч гуталтай болж би гэдэг хүн аль ч үгүй хоцров . эгч гуталтай болж би гэдэг хүн аль ч үгүй хоцров .\n",
    "-----------------\n",
    "үүний хамт эдгээр технологийн анагаах ухааны асуудалд дүн шинжилгээ хийнэ . үүний хамт эдгээр технологийн анагаах ухааны асуудалд дүн шинжилгээ хийнэ .\n",
    "-----------------\n",
    "энэ бол гэр бүл нь хориотой сүмийн дэргэд байдаггүй хүмүүсийнх . энэ бол гэр бүл нь хориотой сүмийн дэргэд байдаггүй хүмүүсийнх .\n",
    "-----------------\n",
    "тэгэхлээр домог үлгэр худал биш , балар эртний тухай бодит үнэн оршиж байдаг . тэгэхлээр домог үлгэр худал биш , балар эртний тухай бодит үнэн оршиж байдаг .\n",
    "-----------------\n",
    "өчигдөр л мөнгөө аваад явдаг байж хайран мөнгө . өчигдөр л мөнгөө аваад явдаг байж хайран мөнгө .\n",
    "-----------------\n",
    "пин жа жэнши цолт басали г илгээж гүр сүмд тахиж өчив . пин жан жэнши цолт басали г илгээж гүр сүмд тахиж өчив .\n",
    "-----------------\n",
    "түүнээс гадна бүлэглэлд христийн шүтлэгт үнэнч гэдгээ нотолсон хаадыг элсүүлэн авдаг . түүнээс гадна бүлэглэлд христийн шүтлэгт үнэнч гэдгээ нотолсон хаадыг элсүүлэн авдаг .\n",
    "-----------------\n",
    "таван жилийн дараа наполеон дараагийн бөгөөд эцсийн алхмаа хийж , францын эзэн хаанаар өргөмжлөгдсөн юм . таван жилийн дараа наполеон дараагийн бөгөөд эцсийн алхмаа хийж , францын эзэн хаанаар өргөмжлөгдсөн юм .\n",
    "-----------------\n",
    "ядаж тэгээд манай бэрүү гэж хоёр сүрхий толгойнууд бий . ядаж тэгээд манай бэрүүд гэж хоёр сүрхий толгойнууд бий .\n",
    "-----------------\n",
    "энэ үеэр бела ван өөрт нь туслахаар ирсэн австрийн герцог фридрих бабенбергтэй маргасан байна . энэ үеэр бела ван өөрт нь туслахаар ирсэн австрийн герцог фридрих бабенбергтэй маргасан байна .\n",
    "-----------------\n",
    "амжилттай ажиллаж буй хөрөнгө оруулалтыг дэмжих байгууллагууд дараах гурван гол чиглэлийн стратегийг хэрэгжүүлж байна . амжилттай ажиллаж буй хөрөнгө оруулалтыг дэмжих байгууллагууд дараах гурван гол чиглэлийн стратегийг хэрэгжүүлж байна .\n",
    "-----------------\n",
    "хэн ч үүнийг аугаа харилцагч гэж нэрлэгддэг хүнээс илүү гар хийгээгүй . хэн ч үүнийг аугаа харилцагч гэж нэрлэгддэг хүнээс илүү гарч хийгээгүй .\n",
    "-----------------\n",
    "энэ нь бэлчээрийн хүртээмж , чанарыг муутгахын зэрэгцээ тоосжилт ихэсч агаарын чанарт сөргөөр нөлөөлж байна . энэ нь бэлчээрийн хүртээмж , чанарыг муутгахын зэрэгцээ тоосжилт ихэсч агаарын чанарт сөргөөр нөлөөлж байна .\n",
    "-----------------\n",
    "орчин үед ялангуяа гайхамшигт зүйлийн агуулга хэлбэр улам ихсэж байна . орчин үед ялангуяа гайхамшигт зүйлийн агуулга хэлбэр улам ихсэж байна .\n",
    "-----------------\n",
    "улсын их хурал тус хуулийг батлахдаа хаалттай хуралдаанаар хэлэлцсэн сэтгэл зовниулсан асуудал юм . улсын их хурал тус хуулийг батлахдаа хаалттай хуралдаанаар хэлэлцсэн сэтгэл зовниулсан асуудал юм .\n",
    "-----------------\n",
    "нутгийн зүүн үрсэд нөмрөгийн дцг т жижиг популяци бий . нутгийн зүүн бүсэд нөмрөгийн дцг т жижиг популяци бий .\n",
    "-----------------\n",
    "тэгэх боломж , нөхцөл ч бага байсан биз ээ . тэгэх боломж , нөхцөл ч бага байсан биз ээ .\n",
    "-----------------\n",
    "эдгээр асуудлуудад хамаарах нарийвчилсан мэдээллийг хүснэгт . 1 т үзүүлэв . эдгээр асуудлуудад хамаарах нарийвчилсан мэдээллийг хүснэгт . 1 т үзүүлэв .\n",
    "-----------------\n",
    "хичээлийн мөн чанараас улбаалан аливаа асуултанд хялбар , түргэн хариулт байхгүй гэдэг нь тодорхой . хичээлийн мөн чанараас улбаалан аливаа асуултанд хялбар , түргэн хариулт байхгүй гэдэг нь тодорхой .\n",
    "-----------------\n",
    "европын анагаах ухааны хичээлийн сургалтын хөтөлбөр . европын анагаах ухааны хичээлийн сургалтын хөтөлбөр .\n",
    "-----------------\n",
    "эрх мэдэл хуваарилах зарчмыг сэргээснээр хариуцлагыг чангатгаж , бодлогын хэрэгжилтийг сайжруулна . эрх мэдэл хуваарилах зарчмыг сэргээснээр хариуцлагыг чангатгаж , бодлогын хэрэгжилтийг сайжруулна .\n",
    "-----------------\n",
    "эдгээр нь дараах 3 зүйлийг тодорхойлоход оршино . эдгээр нь дараах 3 зүйлийг тодорхойлоход оршино .\n",
    "-----------------\n",
    "тэгээд үл сүсэглэгч ичингүйрчүхүй: аллах нүгэлтнүүдийг шулуун замаар эс хөтөлмүй ! тэгээд үл сүсэглэгч ичингүйрчүхүй: аллах нүгэлтнүүдийг шулуун замаар эс хөтөлмүй !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = MonBigPurple(fineTuningClass,'./RRRRRRRRRRR')\n",
    "test_spell = open(\"test_spell_error.txt\", \"r\")\n",
    "test_spell = test_spell.read().split(\"\\n\")\n",
    "OUT = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(test_spell))):\n",
    "    temp = process_list(test_spell[i])\n",
    "    temp = colon(temp)\n",
    "    predict = te.predict(temp)\n",
    "    for j in predict:\n",
    "        t = j['t']\n",
    "        ii = j['i']\n",
    "\n",
    "        temp[ii] = t\n",
    "    temp = ' '.join(temp)\n",
    "    temp = temp.replace(' : ',': ')\n",
    "    OUT.append(temp)\n",
    "    print('-----------------')    \n",
    "    print(temp)\n",
    "    print(test_spell[i])\n",
    "    print('-----------------')    \n",
    "\n",
    "with open('test_spell_error_out.txt', 'w') as f:\n",
    "    for item in OUT:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT = []\n",
    "# for i in tqdm(monBigTool.getMASK()):\n",
    "#     sen = []\n",
    "#     labels = []\n",
    "#     for ss in i['sen']:\n",
    "#         if ss == '<>':\n",
    "#             i['word'].pop(0)\n",
    "#             sen.append(i['word'].pop(0))\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [1] * count\n",
    "#         else:\n",
    "#             sen.append(ss)\n",
    "#             count = sen[-1]\n",
    "#             count = len(tokenizer(count)['input_ids'])\n",
    "#             labels += [0] * count\n",
    "#     OUT.append({\n",
    "#         'sen':sen,\n",
    "#         'labels':labels\n",
    "#     })\n",
    "# # 保存OUT\n",
    "# import json\n",
    "# with open('sentencePair.json','w') as f:\n",
    "#     json.dump(OUT,f)\n",
    "\n",
    "# # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)\n",
    "#     # 读取\n",
    "# import json\n",
    "# with open('sentencePair.json','r') as f:\n",
    "#     OUT = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
