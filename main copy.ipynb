{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T07:40:49.065336Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "MODELNAME = './mongolian'\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from MonBigTool import IS_levenshtein_distance_and_operations\n",
    "from MonBigTool import levenshtein_distance_and_operations\n",
    "from MonBigTool import MonBigTool,MASKmodel\n",
    "\n",
    "mASKmodel = MASKmodel(MODELNAME)\n",
    "monBigTool = MonBigTool()\n",
    "WordsDict = monBigTool.getWordsDict()\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "MASK =  monBigTool.getMASK()\n",
    "for i in tqdm(MASK):\n",
    "\n",
    "    sen = []\n",
    "    _labels = []\n",
    "    token_labels = []\n",
    "    for j in i['sen']:\n",
    "        if j == '<>':\n",
    "            i['word'].pop(0)\n",
    "            sen.append( i['word'].pop(0))\n",
    "            _labels.append(1)\n",
    "        else:\n",
    "            sen.append(j)\n",
    "            _labels.append(0)\n",
    "    \n",
    "    sentence = ' '.join(sen)\n",
    "\n",
    "    # sentence_length = tokenizer(sentence)['input_ids']\n",
    "    # tokens = tokenizer.convert_ids_to_tokens(sentence_length)\n",
    "    # tokens = [token.replace('▁', '') for token in tokens]\n",
    "    # tokens = [token.replace('#', '') for token in tokens]\n",
    "\n",
    "    # tokens2 = ['[CLS]']\n",
    "    token_labels = [0]\n",
    "    for j in range(len(sen)):\n",
    "        aAaaa = tokenizer.tokenize(sen[j])\n",
    "        token_labels += ([_labels[j]] * len(aAaaa))\n",
    "        # aAaaa = [token.replace('▁', '') for token in aAaaa]\n",
    "        # aAaaa = [token.replace('#', '') for token in aAaaa]\n",
    "        # tokens2 += aAaaa\n",
    "    # tokens2.append('[SEP]')\n",
    "    token_labels.append(0)\n",
    "\n",
    "    if len(token_labels) > 32:\n",
    "        continue\n",
    "    texts.append(sentence)\n",
    "    labels.append(token_labels)\n",
    "\n",
    "    \n",
    "    # print(sen)\n",
    "    # print(labels)\n",
    "\n",
    "    # 判断 token 和 token2 是否一致\n",
    "    # if len(tokens2) != len(tokenlabels):\n",
    "    #     print('error')\n",
    "    #     print(' '.join(sen))\n",
    "    #     print(tokens)\n",
    "    #     print(tokens2)\n",
    "    #     print('---------------------')\n",
    "    #     continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "    # 读取\n",
    "\n",
    "class SpellingErrorDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # 对文本进行编码\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        # 替换 32000 为 0\n",
    "        encoding['input_ids'][encoding['input_ids'] == 32000] = 0\n",
    "        # print(encoding['input_ids'].flatten().max())\n",
    "        # print(encoding['input_ids'].flatten().min())\n",
    "        # print(sentence)\n",
    "        # print(token_labels)\n",
    "        # print('---------------------')\n",
    "        # 将标签对齐到编码的长度\n",
    "        labels = labels + [0] * (self.max_len - len(labels))\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "tokenizer = AutoTokenizer.from_pretrained('tugstugi/bert-large-mongolian-uncased', use_fast=False)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 创建分词器\n",
    "\n",
    "\n",
    "dataset = SpellingErrorDataset(texts, labels, tokenizer, max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "test_dataset = dataset\n",
    "\n",
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 加载预训练模型，指定要进行分类的类别数量\n",
    "model = BertForTokenClassification.from_pretrained('tugstugi/bert-large-mongolian-uncased', num_labels=2)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出目录\n",
    "    num_train_epochs=3,              # 总的训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=64,   # 每个设备的评估批次大小\n",
    "    warmup_steps=500,                # 预热步数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    ")\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 要训练的模型\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=train_dataset,         # 训练数据集\n",
    "    eval_dataset=test_dataset            # 评估数据集\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
